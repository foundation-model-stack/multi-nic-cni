{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"concept/","text":"Concept","title":"Concept"},{"location":"concept/#concept","text":"","title":"Concept"},{"location":"concept/architecture/","text":"Architecture Multi-NIC CNI operator is composed of three main components: controller, daemon, and CNI. The controller implements Operator SDK to create and run a reconcile loop over the CNI custom resource that is MultiNicNetwork, HostInterfaces, CIDR, and IPPool via kube-apiserver. The controller periodically gets interface information from host networks by calling discovery protocol to the daemon and records in HostInterface resource. The controller creates Multus's NetworkAttachmentDefinition and dependent custom resources of main plugin CNI (e.g., sriovnetworknodepolicies of SR-IOV CNI) from MultiNicNetwork 's spec. The generation of CIDR and IPPool , L3 route configuration, and IP allocation/deallocation can be found in Multi-NIC IPAM Plugin . The CNI component is delegated by Multus CNI. It communicates with daemon to select a set of master interfaces according to the policy if defined. If the built-in IPAM is used, it will request for IPs regarding these selected masters. Otherwise, it will delegate the common IPAM plugin to get IP address for each selected NIC. After getting IP addresses, it will delegate the common main plugin (e.g., ipvlan, macvlan, sriov) to configure each additional interface. Note: In addition to CNI-related resource, controller also run a reconcile loop over the Config custom resource to manage daemon and CNI components","title":"Architecture"},{"location":"concept/architecture/#architecture","text":"Multi-NIC CNI operator is composed of three main components: controller, daemon, and CNI. The controller implements Operator SDK to create and run a reconcile loop over the CNI custom resource that is MultiNicNetwork, HostInterfaces, CIDR, and IPPool via kube-apiserver. The controller periodically gets interface information from host networks by calling discovery protocol to the daemon and records in HostInterface resource. The controller creates Multus's NetworkAttachmentDefinition and dependent custom resources of main plugin CNI (e.g., sriovnetworknodepolicies of SR-IOV CNI) from MultiNicNetwork 's spec. The generation of CIDR and IPPool , L3 route configuration, and IP allocation/deallocation can be found in Multi-NIC IPAM Plugin . The CNI component is delegated by Multus CNI. It communicates with daemon to select a set of master interfaces according to the policy if defined. If the built-in IPAM is used, it will request for IPs regarding these selected masters. Otherwise, it will delegate the common IPAM plugin to get IP address for each selected NIC. After getting IP addresses, it will delegate the common main plugin (e.g., ipvlan, macvlan, sriov) to configure each additional interface. Note: In addition to CNI-related resource, controller also run a reconcile loop over the Config custom resource to manage daemon and CNI components","title":"Architecture"},{"location":"concept/multi-nic-ipam/","text":"Common NAT-bypassing network solution without underlay infrastructure dependency: L3 IPVLAN with neighbor routing entries Multi-NIC CNI Components VPC Cluster Requirements IPAM Configuration Workflows Interface Discovery CIDR Generation and L3 Route Auto-configuration / Clean up CIDR Generation L3 Route Auto-configuration IP Allocation / Deallocation Common NAT-bypassing network solution without underlay infrastructure dependency: L3 IPVLAN with neighbor routing entries The target is to attach secondary network interface cards at hosts to the container pods and bypass the costly network address translation to efficiently deliver network packets between pods on different hosts. IPVLAN is a software multiplexing tool that exposes Pod packet and Pod IP directly to master interface (NIC) on the host. In most cases, Pod IPs are not routable by the underlay virtual Cloud infrastructure. Configuring a neighbor route entry (L3 routes) on the host will enable communication between endpoints on the different hosts. Multi-NIC CNI computes a specific CIDR range for each interface and each host incrementally from the user-defined global subnet limiting by defined block sizes. In the above example, as the global subnet is 192.168.0.0/16 with 2 interface bits and 6 host bits, the IP addresses assigned to the first master interface (eth1) will start with 192.168.0.x - 192.168.63.x while that assigned to the second one (eth2) will start with 192.168.64.x - 192.168.127.x. The IP addresses assigned to the first master interface in the first host are further specified to the range 192.168.1.0 - 192.168.1.255. The first and the last addresses are reserved for network address and broadcast address to be managed later. With this blocking range of IP, the L3 routes on each host and interface can be configured without conflict. Multi-NIC CNI Components The built-in Multi-NIC IPAM and L3 route auto-configuration performs by collaboration of the controller with Multi-NIC CNI and Multi-NIC daemon . The CNI uses orchrestrator storage to keep data synchronize by defining new three custom resources: NAME APIVERSION NAMESPACED KIND cidrs multinic.fms.io/v1 false CIDR hostinterfaces multinic.fms.io/v1 false HostInterface ippools multinic.fms.io/v1 false IPPool CIDR for recording the computed CIDRs. This resource is created or updated when MultiNicNetwork is created or updated with IPAM type multi-nic-ipam . HostInterface is updated HostInterface for keeping discoveried host interface data. This resource is updated if there is a change of host interfaces checked every minute. IPPool for managing IP allocation/deallocation. This resource is created or updated at the same time when CIDR is created or updated. VPC Cluster Requirements main plugin support (e.g., kernel version >= 4.2 for ipvlan) For L3 mode, - enable allowing IP spoofing for each attached interface - security group - allow target global subnet on secondary subnets - allow daemon port (default: 11000) on primary subnet IPAM Configuration In addition to global subnet and designated masterNets , Multi-NIC IPAM requires the following arguments to compute CIDR for each host and each interface. Argument|Description|Value|Remarks ---|---|---|--- vlanMode|mode for creating ipvlan|l2, l3, l3s|For ls3 and l3s mode, the cni will automatically create corresponding host routes in level 3 hostBlock|number of address bits for host indexing| int (n) | the number of assignable host = 2^n interfaceBlock|number of address bits for interface indexing| int (m) | the number of assignable interfaces = 2^m excludeCIDRs|list of ip range (CIDR) to exclude|list of string| example of IPAM-related spec in MultiNicNetwork resource: spec: subnet: \"192.168.0.0/16\" ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 6, \"interfaceBlock\": 2, \"vlanMode\": \"l3\" } multiNICIPAM: true masterNets: - \"10.0.1.0/24\" - \"10.0.2.0/24\" Workflows Interface Discovery CIDR Generation and L3 Route Auto-configuration / Clean up CIDR Generation The current version of CIDR is based on IPv4 which contains 32 bits. Given, hosts=[\"Host1\", \"Host2\"] subnet=192.168.0.0/16 hostBlock=6 interfaceBlock=2 masterNets=[\"10.0.1.0/24\", \"10.0.2.0/24\"] Host1 and Host2 are assigned with index 0 and 1 respectively and, at the same time, interfaces with 10.0.1.0/24 and with 10.0.2.0/24 are assigned with index 0 and 1 respectively. The first 16 bits are reserved for global subnet. The next 2 bits are reserved for interface index. The next 6 bits are reserved for host index. The rest 8 bits are for pod-specific IP address. Accordingly, the pod CIDR for Host1 with network addresss 10.0.1.0/24 is 192.168.0.0/24. (00|000000 from bit 17 to 24) the pod CIDR for Host1 with network addresss 10.0.2.0/24 is 192.168.64.0/24. (01|000000 from bit 17 to 24) the pod CIDR for Host2 with network addresss 10.0.1.0/24 is 192.168.1.0/24. (00|000001 from bit 17 to 24) the pod CIDR for Host2 with network addresss 10.0.2.0/24 is 192.168.65.0/24. (01|000001 from bit 17 to 24) L3 Route Auto-configuration If the vlanMode is set to l3 or l3s, the CNI will configure route table on Pod and Host when the CIDR resource is created as follows. On Pod, the vlan CIDR of each interface is set up to interface block. On Host, the next-hop route is set up to host block. For example, routes configured regarding the above example, # On Pod index 1 at Host1 (IPs = 192.168.0.1, 192.168.64.1) > ip route 192.168.0.0/18 dev net1-0 proto kernel scope link src 192.168.0.1 192.168.64.0/18 dev net1-1 proto kernel scope link src 192.168.64.1 # On Host1 (IPs = 10.0.1.1, 10.0.2.1) when Host2 has IPs 10.0.1.2, 10.0.2.2, > ip route 192.168.1.0/24 via 10.0.1.2 dev eth1 192.168.65.0/24 via 10.0.2.2 dev eth2 IP Allocation / Deallocation The CNI will send a request to daemon running on the deployed host to get a set of IP addresses regarding a set of the interface names. This is a locked operation within the daemon function to prevent allocating the same IP address to different pods at the same time.","title":"Multi nic ipam"},{"location":"concept/multi-nic-ipam/#common-nat-bypassing-network-solution-without-underlay-infrastructure-dependency-l3-ipvlan-with-neighbor-routing-entries","text":"The target is to attach secondary network interface cards at hosts to the container pods and bypass the costly network address translation to efficiently deliver network packets between pods on different hosts. IPVLAN is a software multiplexing tool that exposes Pod packet and Pod IP directly to master interface (NIC) on the host. In most cases, Pod IPs are not routable by the underlay virtual Cloud infrastructure. Configuring a neighbor route entry (L3 routes) on the host will enable communication between endpoints on the different hosts. Multi-NIC CNI computes a specific CIDR range for each interface and each host incrementally from the user-defined global subnet limiting by defined block sizes. In the above example, as the global subnet is 192.168.0.0/16 with 2 interface bits and 6 host bits, the IP addresses assigned to the first master interface (eth1) will start with 192.168.0.x - 192.168.63.x while that assigned to the second one (eth2) will start with 192.168.64.x - 192.168.127.x. The IP addresses assigned to the first master interface in the first host are further specified to the range 192.168.1.0 - 192.168.1.255. The first and the last addresses are reserved for network address and broadcast address to be managed later. With this blocking range of IP, the L3 routes on each host and interface can be configured without conflict.","title":"Common NAT-bypassing network solution without underlay infrastructure dependency: L3 IPVLAN with neighbor routing entries"},{"location":"concept/multi-nic-ipam/#multi-nic-cni-components","text":"The built-in Multi-NIC IPAM and L3 route auto-configuration performs by collaboration of the controller with Multi-NIC CNI and Multi-NIC daemon . The CNI uses orchrestrator storage to keep data synchronize by defining new three custom resources: NAME APIVERSION NAMESPACED KIND cidrs multinic.fms.io/v1 false CIDR hostinterfaces multinic.fms.io/v1 false HostInterface ippools multinic.fms.io/v1 false IPPool CIDR for recording the computed CIDRs. This resource is created or updated when MultiNicNetwork is created or updated with IPAM type multi-nic-ipam . HostInterface is updated HostInterface for keeping discoveried host interface data. This resource is updated if there is a change of host interfaces checked every minute. IPPool for managing IP allocation/deallocation. This resource is created or updated at the same time when CIDR is created or updated.","title":"Multi-NIC CNI Components"},{"location":"concept/multi-nic-ipam/#vpc-cluster-requirements","text":"main plugin support (e.g., kernel version >= 4.2 for ipvlan) For L3 mode, - enable allowing IP spoofing for each attached interface - security group - allow target global subnet on secondary subnets - allow daemon port (default: 11000) on primary subnet","title":"VPC Cluster Requirements"},{"location":"concept/multi-nic-ipam/#ipam-configuration","text":"In addition to global subnet and designated masterNets , Multi-NIC IPAM requires the following arguments to compute CIDR for each host and each interface. Argument|Description|Value|Remarks ---|---|---|--- vlanMode|mode for creating ipvlan|l2, l3, l3s|For ls3 and l3s mode, the cni will automatically create corresponding host routes in level 3 hostBlock|number of address bits for host indexing| int (n) | the number of assignable host = 2^n interfaceBlock|number of address bits for interface indexing| int (m) | the number of assignable interfaces = 2^m excludeCIDRs|list of ip range (CIDR) to exclude|list of string| example of IPAM-related spec in MultiNicNetwork resource: spec: subnet: \"192.168.0.0/16\" ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 6, \"interfaceBlock\": 2, \"vlanMode\": \"l3\" } multiNICIPAM: true masterNets: - \"10.0.1.0/24\" - \"10.0.2.0/24\"","title":"IPAM Configuration"},{"location":"concept/multi-nic-ipam/#workflows","text":"","title":"Workflows"},{"location":"concept/multi-nic-ipam/#interface-discovery","text":"","title":"Interface Discovery"},{"location":"concept/multi-nic-ipam/#cidr-generation-and-l3-route-auto-configuration-clean-up","text":"","title":"CIDR Generation and L3 Route Auto-configuration / Clean up"},{"location":"concept/multi-nic-ipam/#cidr-generation","text":"The current version of CIDR is based on IPv4 which contains 32 bits. Given, hosts=[\"Host1\", \"Host2\"] subnet=192.168.0.0/16 hostBlock=6 interfaceBlock=2 masterNets=[\"10.0.1.0/24\", \"10.0.2.0/24\"] Host1 and Host2 are assigned with index 0 and 1 respectively and, at the same time, interfaces with 10.0.1.0/24 and with 10.0.2.0/24 are assigned with index 0 and 1 respectively. The first 16 bits are reserved for global subnet. The next 2 bits are reserved for interface index. The next 6 bits are reserved for host index. The rest 8 bits are for pod-specific IP address. Accordingly, the pod CIDR for Host1 with network addresss 10.0.1.0/24 is 192.168.0.0/24. (00|000000 from bit 17 to 24) the pod CIDR for Host1 with network addresss 10.0.2.0/24 is 192.168.64.0/24. (01|000000 from bit 17 to 24) the pod CIDR for Host2 with network addresss 10.0.1.0/24 is 192.168.1.0/24. (00|000001 from bit 17 to 24) the pod CIDR for Host2 with network addresss 10.0.2.0/24 is 192.168.65.0/24. (01|000001 from bit 17 to 24)","title":"CIDR Generation"},{"location":"concept/multi-nic-ipam/#l3-route-auto-configuration","text":"If the vlanMode is set to l3 or l3s, the CNI will configure route table on Pod and Host when the CIDR resource is created as follows. On Pod, the vlan CIDR of each interface is set up to interface block. On Host, the next-hop route is set up to host block. For example, routes configured regarding the above example, # On Pod index 1 at Host1 (IPs = 192.168.0.1, 192.168.64.1) > ip route 192.168.0.0/18 dev net1-0 proto kernel scope link src 192.168.0.1 192.168.64.0/18 dev net1-1 proto kernel scope link src 192.168.64.1 # On Host1 (IPs = 10.0.1.1, 10.0.2.1) when Host2 has IPs 10.0.1.2, 10.0.2.2, > ip route 192.168.1.0/24 via 10.0.1.2 dev eth1 192.168.65.0/24 via 10.0.2.2 dev eth2","title":"L3 Route Auto-configuration"},{"location":"concept/multi-nic-ipam/#ip-allocation-deallocation","text":"The CNI will send a request to daemon running on the deployed host to get a set of IP addresses regarding a set of the interface names. This is a locked operation within the daemon function to prevent allocating the same IP address to different pods at the same time.","title":"IP Allocation / Deallocation"},{"location":"concept/policy/","text":"Attachment Policy To apply attachment policy, the key attachPolicy need to be specified in MultiNicNetwork and specific arguments can be added specific to Pod annotation (if needed). # MultiNicNetwork spec: attachPolicy: strategy: none|costOpt|perfOpt|devClass Policy Description Status none (default) Apply all NICs in the pool implemented costOpt provide target ideal bandwidth with minimum cost based on HostInterface spec and status TODO perfOpt provide target ideal bandwidth with most available NICs set based on HostInterface spec and status TODO devClass give preference for a specific class of NICs based on DeviceClass custom resource implemented Annotation (CNIArgs) Description Status nics fixed number of interfaces (none, DeviceClass strategy) implemented master fixed interface names (none strategy) implemented target overridden target bandwidth (CostOpt, PerfOpt strategy) TODO class preferred device class (DeviceClass strategy) implemented None Strategy (none) When none strategy is set or no strategy is set, the Multi-NIC daemon will basically attach all secondary interfaces listed in HostInterface custom resource to the Pod. # MultiNicNetwork spec: attachPolicy: strategy: none However, pod can be further annotated to apply only a subset of secondary interfaces with a specific number or name list. For example, - attach only one secondary interface metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"nics\": 1 } }] attach with the secondary interface name eth1 metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"master\": [eth1] } }] If both arguments (nics and master) are applied at the same time, the master argument will be applied. DeviceClass Strategy (devClass) When devClass strategy is, the Multi-NIC daemon will be additionally aware of class argument specifed in the pod annotation as a filter. # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"class\": \"highspeed\" \"nics\": 1 } }] With the above annotation, one secondary interface that falls into highspeed class defined by DeviceClass will be attached to the Pod. The DeviceClass resource is composed of a list of vendor and product identifiers as below example. # DeviceClass example apiVersion: multinic.fms.io/v1 kind: DeviceClass metadata: name: highspeed spec: ids: - vendor: \"15b3\" products: - \"1019\" - vendor: \"1d0f\" products: - \"efa0\" - \"efa1\"","title":"Attachment Policy"},{"location":"concept/policy/#attachment-policy","text":"To apply attachment policy, the key attachPolicy need to be specified in MultiNicNetwork and specific arguments can be added specific to Pod annotation (if needed). # MultiNicNetwork spec: attachPolicy: strategy: none|costOpt|perfOpt|devClass Policy Description Status none (default) Apply all NICs in the pool implemented costOpt provide target ideal bandwidth with minimum cost based on HostInterface spec and status TODO perfOpt provide target ideal bandwidth with most available NICs set based on HostInterface spec and status TODO devClass give preference for a specific class of NICs based on DeviceClass custom resource implemented Annotation (CNIArgs) Description Status nics fixed number of interfaces (none, DeviceClass strategy) implemented master fixed interface names (none strategy) implemented target overridden target bandwidth (CostOpt, PerfOpt strategy) TODO class preferred device class (DeviceClass strategy) implemented","title":"Attachment Policy"},{"location":"concept/policy/#none-strategy-none","text":"When none strategy is set or no strategy is set, the Multi-NIC daemon will basically attach all secondary interfaces listed in HostInterface custom resource to the Pod. # MultiNicNetwork spec: attachPolicy: strategy: none However, pod can be further annotated to apply only a subset of secondary interfaces with a specific number or name list. For example, - attach only one secondary interface metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"nics\": 1 } }] attach with the secondary interface name eth1 metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"master\": [eth1] } }] If both arguments (nics and master) are applied at the same time, the master argument will be applied.","title":"None Strategy (none)"},{"location":"concept/policy/#deviceclass-strategy-devclass","text":"When devClass strategy is, the Multi-NIC daemon will be additionally aware of class argument specifed in the pod annotation as a filter. # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"class\": \"highspeed\" \"nics\": 1 } }] With the above annotation, one secondary interface that falls into highspeed class defined by DeviceClass will be attached to the Pod. The DeviceClass resource is composed of a list of vendor and product identifiers as below example. # DeviceClass example apiVersion: multinic.fms.io/v1 kind: DeviceClass metadata: name: highspeed spec: ids: - vendor: \"15b3\" products: - \"1019\" - vendor: \"1d0f\" products: - \"efa0\" - \"efa1\"","title":"DeviceClass Strategy (devClass)"},{"location":"contributor/","text":"Contributing","title":"Contributing"},{"location":"contributor/#contributing","text":"","title":"Contributing"},{"location":"contributor/local_build_push/","text":"Locally Build and Deploy Build images 1. Build CNI operator Set IMAGE_REGISTRY and VERSION environment to target image repository for operator bash export IMAGE_REGISTRY=<registry> export VERSION=<version> For private image registry, follow these additional steps to add image-pulling secret Put your secret for pulling operator image ( operator-secret.yaml ) to the secret folder bash mv operator-secret.yaml config/secret Run script to update relevant kustomization files bash export OPERATOR_SECRET_NAME=$(cat config/secret/operator-secret.yaml|yq .metadata.name) make operator-secret Build and push operator image bash go mod tidy make docker-build docker-push Build and push bundle image (optional) bash make bundle make bundle-build bundle-push To test the bundle, run bash operator-sdk run bundle ${IMAGE_REGISTRY}/multi-nic-cni-bundle:v${VERSION} 2. Build CNI daemon Set IMAGE_REGISTRY and VERSION environment to target image repository for daemon bash export IMAGE_REGISTRY=<registry> export VERSION=<version> For private image registry, follow these additional steps to add image-pulling secret Put your secret for pulling daemon image ( daemon-secret.yaml ) to the secret folder bash mv daemon-secret.yaml config/secret Run script to update relevant kustomization files bash export DAEMON_SECRET_NAME=$(cat config/secret/daemon-secret.yaml|yq .metadata.name) make daemon-secret Build and push daemon image bash # build environment: # Linux systems with netlink library cd daemon go mod tidy make docker-build-push This will also build the cni binary and copy the built binary to daemon component. Install operator bash make deploy ## Uninstall operator bash make undeploy","title":"Locally Build and Deploy"},{"location":"contributor/local_build_push/#locally-build-and-deploy","text":"","title":"Locally Build and Deploy"},{"location":"contributor/local_build_push/#build-images","text":"","title":"Build images"},{"location":"contributor/local_build_push/#1-build-cni-operator","text":"Set IMAGE_REGISTRY and VERSION environment to target image repository for operator bash export IMAGE_REGISTRY=<registry> export VERSION=<version> For private image registry, follow these additional steps to add image-pulling secret Put your secret for pulling operator image ( operator-secret.yaml ) to the secret folder bash mv operator-secret.yaml config/secret Run script to update relevant kustomization files bash export OPERATOR_SECRET_NAME=$(cat config/secret/operator-secret.yaml|yq .metadata.name) make operator-secret Build and push operator image bash go mod tidy make docker-build docker-push Build and push bundle image (optional) bash make bundle make bundle-build bundle-push To test the bundle, run bash operator-sdk run bundle ${IMAGE_REGISTRY}/multi-nic-cni-bundle:v${VERSION}","title":"1. Build CNI operator"},{"location":"contributor/local_build_push/#2-build-cni-daemon","text":"Set IMAGE_REGISTRY and VERSION environment to target image repository for daemon bash export IMAGE_REGISTRY=<registry> export VERSION=<version> For private image registry, follow these additional steps to add image-pulling secret Put your secret for pulling daemon image ( daemon-secret.yaml ) to the secret folder bash mv daemon-secret.yaml config/secret Run script to update relevant kustomization files bash export DAEMON_SECRET_NAME=$(cat config/secret/daemon-secret.yaml|yq .metadata.name) make daemon-secret Build and push daemon image bash # build environment: # Linux systems with netlink library cd daemon go mod tidy make docker-build-push This will also build the cni binary and copy the built binary to daemon component.","title":"2. Build CNI daemon"},{"location":"contributor/local_build_push/#install-operator","text":"bash make deploy ## Uninstall operator bash make undeploy","title":"Install operator"},{"location":"user/","text":"User Guide","title":"User Guide"},{"location":"user/#user-guide","text":"","title":"User Guide"},{"location":"user/troubleshooting/","text":"Troubleshooting Set common variables to be used for troubleshooting: export FAILED_POD= # pod that fails to run export FAILED_POD_NAMESPACE= # namespace where the failed pod is supposed to run export FAILED_NODE= # node where pod is deployed export FAILED_NODE_IP = # IP of FAILED_NODE export MULTI_NIC_NAMESPACE= # namespace where multi-nic cni operator is deployed, default=multi-nic-cni-operator Pod failed to start Pod stays pending in ContainerCreating status. Get more information from describe kubectl describe $FAILED_POD -n $FAILED_POD_NAMESPACE FailedCreatePodSandBox CNI binary not found IPAM ExecAdd failed CNI binary not exist The binary file of CNI is not in the expected location read by Multus. The expected location can be found in Multus daemonset as below. kubectl get ds $(kubectl get ds -A\\ |grep multus|head -n 1|awk '{printf \"%s -n %s\", $2, $1}') -ojson\\ |jq .spec.template.spec.volumes Example output: [ ... { \"hostPath\": { \"path\": \"/var/lib/cni/bin\", \"type\": \"\" }, \"name\": \"cnibin\" }, ... ] The expected location is in hostPath of cnibin . - missing multi-nic/multi-nic-ipam CNI The CNI directory is probably mounted to a wrong location in the configs.multinic.fms.io CR. bash kubectl edit config.multinic multi-nicd -n $MULTI_NIC_NAMESPACE Modify mount path ( hostpath attribute ) in spec.daemon.mounts of cnibin to the target location above. - missing other CNI such as ipvlan The missing CNI may not be supported. IPAM ExecAdd failed failed to load netconf The configuration cannot be loaded. This is delegated CNI (such as IPVLAN) issue. Find more details from CNI log . failed to request ip Response nothing Get more information from HostInterface CR. bash kubectl get HostInterface $FAILED_NODE -oyaml hostinterfaces.multinic.fms.io \"FAILED_NODE\" not found, check HostInterface is not be created. no interfaces in .spec.interfaces , check HostInterface does not show the secondary interfaces. check whether it reaches CIDR block limit, confirm no available IP address other cases, find more details from multi-nicd log other CNI plugin (such as aws-vpc-cni, sr-iov) failure, check each CNI log. aws-vpc-cni: /host/var/log/aws-routed-eni HostInterface not created There are a couple of reasons that the HostInterface is not created. First check the multi-nicd DaemonSet. kubectl get ds multi-nicd -n $MULTI_NIC_NAMESPACE -oyaml daemonsets.apps \"multi-nicd\" not found Check whether no config.multinic.fms.io deployed in the cluster. bash kubectl get config.multinic multi-nicd -n $MULTI_NIC_NAMESPACE If no config.multinic.fms.io deployed, see Deploy multi-nicd The node has taint that the daemon is not tolerate. bash kubectl get nodes $FAILED_NODE -o json|jq -r .spec.taints To tolerate the taint, add the tolerate manually to the multi-nicd DaemonSet. bash kubectl edit $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') More detail: taints and torelations Other cases, check controller log No secondary interfaces in HostInterface The HostInterface is created but there is no interface listed in the custom resource. Check whether the controller can communicate with multi-nicd. kubectl logs --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE -c manager| grep Join| grep $FAILED_NODE_IP If no line shown up and the full controller log keep printing Fail to create hostinterface ... cannot update interfaces: Get \"<node IP>/interface\": dial tcp <node IP>:11000: i/o timeout , check set required security group rules Other cases, check interfaces at node's host network No available IP address List corresponding Pod CIDR from HostInterface. kubectl get HostInterface $FAILED_NODE -oyaml Check ippools.multinic.fms.io of the corresponding pod CIDR whether the IP address actually reach the limit. If yes, consider changing the host block and interface block in multinicnetworks.multinic.fms.io. Ping failed Check route status in multinicnetworks.multinic.fms.io. kubectl get multinicnetwork.multinic.fms.io multinic-ipvlanl3 -o json\\ | jq -r .status.routeStatus WaitForRoutes : the new cidr is just recomputed and waiting for route update. Failed : some route cannot be applied, need attention. Check multi-nicd log Unknown : some daemon cannot be connected. N/A : there is no L3 configuration applied. Check whether multinicnetwork.multinic.fms.io is defined with L3 mode and cidrs.multinic.fms.io is created. bash kubectl get cidrs.multinic.fms.io Success : check set required security group rules TCP/UDP communication failed. Check whether the multi-nicd detects the other host interfaces. kubectl get po $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') -o json\\ |jq -r .metadata.labels The nubmer in multi-nicd-join should be equal to accumulated number of interfaces from each host in the same zone. Check whether the host secondary interfaces between hosts are connected . If so, try restarting multi-nic-cni controller node to forcefully synchronize host interfaces. Actions Get CNI log bash kubectl exec $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')\\ -- cat /host/var/log/multi-nic-cni.log Get Controller log kubectl logs --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE -c manager Get multi-nicd log kubectl logs $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') Deploy multi-nicd config Restart the controller pod should create the multi-nicd config automatically. kubectl delete po --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE If not, update the controller to the latest image and restart the controller (recommended). Otherwise, deploy config manually. kubectl create -f https://raw.githubusercontent.com/foundation-model-stack/multi-nic-cni/main/config/samples/config.yaml \\ -n $MULTI_NIC_NAMESPACE Set security groups There are four security group rules that must be opened for Multi-nic CNI. 1. outbound/inbound communication within the same security group 2. outbound/inbound communication of Pod networks 3. inbound multi-nicd serving TCP port (default: 11000) Add secondary interfaces Prepare secondary subnets with required security group rules and enable multiple source IPs from a single vNIC (i.e.g, enable IP spoofing on IBM Cloud) Attach the secondary subnets to instance manual attachment: follows Cloud provider instruction by machine-api-operator: updates an image of machine api controller of the provider to support secondary interface on provider spec. Check example commit in modified controller Restart controller kubectl delete --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE Check host secondary interfaces Log in to FAILED_NODE with oc debug node/$FAILED_NODE or using nettools with hostNetwork: true . If secondary interfaces do not exist at the host network, add the secondary interfaces","title":"Troubleshooting"},{"location":"user/troubleshooting/#troubleshooting","text":"Set common variables to be used for troubleshooting: export FAILED_POD= # pod that fails to run export FAILED_POD_NAMESPACE= # namespace where the failed pod is supposed to run export FAILED_NODE= # node where pod is deployed export FAILED_NODE_IP = # IP of FAILED_NODE export MULTI_NIC_NAMESPACE= # namespace where multi-nic cni operator is deployed, default=multi-nic-cni-operator","title":"Troubleshooting"},{"location":"user/troubleshooting/#pod-failed-to-start","text":"Pod stays pending in ContainerCreating status. Get more information from describe kubectl describe $FAILED_POD -n $FAILED_POD_NAMESPACE FailedCreatePodSandBox CNI binary not found IPAM ExecAdd failed","title":"Pod failed to start"},{"location":"user/troubleshooting/#cni-binary-not-exist","text":"The binary file of CNI is not in the expected location read by Multus. The expected location can be found in Multus daemonset as below. kubectl get ds $(kubectl get ds -A\\ |grep multus|head -n 1|awk '{printf \"%s -n %s\", $2, $1}') -ojson\\ |jq .spec.template.spec.volumes Example output: [ ... { \"hostPath\": { \"path\": \"/var/lib/cni/bin\", \"type\": \"\" }, \"name\": \"cnibin\" }, ... ] The expected location is in hostPath of cnibin . - missing multi-nic/multi-nic-ipam CNI The CNI directory is probably mounted to a wrong location in the configs.multinic.fms.io CR. bash kubectl edit config.multinic multi-nicd -n $MULTI_NIC_NAMESPACE Modify mount path ( hostpath attribute ) in spec.daemon.mounts of cnibin to the target location above. - missing other CNI such as ipvlan The missing CNI may not be supported.","title":"CNI binary not exist"},{"location":"user/troubleshooting/#ipam-execadd-failed","text":"failed to load netconf The configuration cannot be loaded. This is delegated CNI (such as IPVLAN) issue. Find more details from CNI log . failed to request ip Response nothing Get more information from HostInterface CR. bash kubectl get HostInterface $FAILED_NODE -oyaml hostinterfaces.multinic.fms.io \"FAILED_NODE\" not found, check HostInterface is not be created. no interfaces in .spec.interfaces , check HostInterface does not show the secondary interfaces. check whether it reaches CIDR block limit, confirm no available IP address other cases, find more details from multi-nicd log other CNI plugin (such as aws-vpc-cni, sr-iov) failure, check each CNI log. aws-vpc-cni: /host/var/log/aws-routed-eni","title":"IPAM ExecAdd failed"},{"location":"user/troubleshooting/#hostinterface-not-created","text":"There are a couple of reasons that the HostInterface is not created. First check the multi-nicd DaemonSet. kubectl get ds multi-nicd -n $MULTI_NIC_NAMESPACE -oyaml daemonsets.apps \"multi-nicd\" not found Check whether no config.multinic.fms.io deployed in the cluster. bash kubectl get config.multinic multi-nicd -n $MULTI_NIC_NAMESPACE If no config.multinic.fms.io deployed, see Deploy multi-nicd The node has taint that the daemon is not tolerate. bash kubectl get nodes $FAILED_NODE -o json|jq -r .spec.taints To tolerate the taint, add the tolerate manually to the multi-nicd DaemonSet. bash kubectl edit $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') More detail: taints and torelations Other cases, check controller log","title":"HostInterface not created"},{"location":"user/troubleshooting/#no-secondary-interfaces-in-hostinterface","text":"The HostInterface is created but there is no interface listed in the custom resource. Check whether the controller can communicate with multi-nicd. kubectl logs --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE -c manager| grep Join| grep $FAILED_NODE_IP If no line shown up and the full controller log keep printing Fail to create hostinterface ... cannot update interfaces: Get \"<node IP>/interface\": dial tcp <node IP>:11000: i/o timeout , check set required security group rules Other cases, check interfaces at node's host network","title":"No secondary interfaces in HostInterface"},{"location":"user/troubleshooting/#no-available-ip-address","text":"List corresponding Pod CIDR from HostInterface. kubectl get HostInterface $FAILED_NODE -oyaml Check ippools.multinic.fms.io of the corresponding pod CIDR whether the IP address actually reach the limit. If yes, consider changing the host block and interface block in multinicnetworks.multinic.fms.io.","title":"No available IP address"},{"location":"user/troubleshooting/#ping-failed","text":"Check route status in multinicnetworks.multinic.fms.io. kubectl get multinicnetwork.multinic.fms.io multinic-ipvlanl3 -o json\\ | jq -r .status.routeStatus WaitForRoutes : the new cidr is just recomputed and waiting for route update. Failed : some route cannot be applied, need attention. Check multi-nicd log Unknown : some daemon cannot be connected. N/A : there is no L3 configuration applied. Check whether multinicnetwork.multinic.fms.io is defined with L3 mode and cidrs.multinic.fms.io is created. bash kubectl get cidrs.multinic.fms.io Success : check set required security group rules","title":"Ping failed"},{"location":"user/troubleshooting/#tcpudp-communication-failed","text":"Check whether the multi-nicd detects the other host interfaces. kubectl get po $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') -o json\\ |jq -r .metadata.labels The nubmer in multi-nicd-join should be equal to accumulated number of interfaces from each host in the same zone. Check whether the host secondary interfaces between hosts are connected . If so, try restarting multi-nic-cni controller node to forcefully synchronize host interfaces.","title":"TCP/UDP communication failed."},{"location":"user/troubleshooting/#actions","text":"","title":"Actions"},{"location":"user/troubleshooting/#get-cni-log","text":"bash kubectl exec $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')\\ -- cat /host/var/log/multi-nic-cni.log","title":"Get CNI log"},{"location":"user/troubleshooting/#get-controller-log","text":"kubectl logs --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE -c manager","title":"Get Controller log"},{"location":"user/troubleshooting/#get-multi-nicd-log","text":"kubectl logs $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')","title":"Get multi-nicd log"},{"location":"user/troubleshooting/#deploy-multi-nicd-config","text":"Restart the controller pod should create the multi-nicd config automatically. kubectl delete po --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE If not, update the controller to the latest image and restart the controller (recommended). Otherwise, deploy config manually. kubectl create -f https://raw.githubusercontent.com/foundation-model-stack/multi-nic-cni/main/config/samples/config.yaml \\ -n $MULTI_NIC_NAMESPACE","title":"Deploy multi-nicd config"},{"location":"user/troubleshooting/#set-security-groups","text":"There are four security group rules that must be opened for Multi-nic CNI. 1. outbound/inbound communication within the same security group 2. outbound/inbound communication of Pod networks 3. inbound multi-nicd serving TCP port (default: 11000)","title":"Set security groups"},{"location":"user/troubleshooting/#add-secondary-interfaces","text":"Prepare secondary subnets with required security group rules and enable multiple source IPs from a single vNIC (i.e.g, enable IP spoofing on IBM Cloud) Attach the secondary subnets to instance manual attachment: follows Cloud provider instruction by machine-api-operator: updates an image of machine api controller of the provider to support secondary interface on provider spec. Check example commit in modified controller","title":"Add secondary interfaces"},{"location":"user/troubleshooting/#restart-controller","text":"kubectl delete --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE","title":"Restart controller"},{"location":"user/troubleshooting/#check-host-secondary-interfaces","text":"Log in to FAILED_NODE with oc debug node/$FAILED_NODE or using nettools with hostNetwork: true . If secondary interfaces do not exist at the host network, add the secondary interfaces","title":"Check host secondary interfaces"}]}