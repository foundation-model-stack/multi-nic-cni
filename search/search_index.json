{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Multi-NIC CNI Multi-NIC CNI is the CNI plugin for secondary networks operating on top of Multus CNI . This CNI offers several key features, outlined below, to help cluster administrators and users simplify the process of enabling high-performance networking. I) Unifying user-managed network definition : User can manage only one network definition for multiple secondary interfaces with a common CNI main plugin such as ipvlan, macvlan, and sr-iov.The Multi-NIC CNI automatically discovers all available secondary interfaces and handles them as a NIC pool. II) Bridging device plugin runtime results and CNI configuration: Multi-NIC CNI can configure CNI of network device in accordance to device plugin allocation results orderly. III) Building-in with several auto-configured CNIs Leveraging advantage point of managing multiple CNIs together with auto-discovery and dynamic interface selection, we built several auto-configured CNIs in the Multi-NIC CNI project including host-interface-local IPAM, multi-config IPAM, multi-gateway routing, and AWS-ipvlan CNI. Check out the project on GitHub \u27a1\ufe0f Multi-NIC CNI . For more insights, please check Medium Blog Post Series KubeCon+CloudNativeCon NA 20222 CNCF-Hosted Co-located Event KubeCon+CloudNativeCon NA 20224 CNCF-Hosted Co-located Event Multi-NIC CNI in Vela IBM Research's AI supercomputer in the cloud The infrastructure powering IBM's Gen AI model development","title":"Multi-NIC CNI"},{"location":"#multi-nic-cni","text":"Multi-NIC CNI is the CNI plugin for secondary networks operating on top of Multus CNI . This CNI offers several key features, outlined below, to help cluster administrators and users simplify the process of enabling high-performance networking. I) Unifying user-managed network definition : User can manage only one network definition for multiple secondary interfaces with a common CNI main plugin such as ipvlan, macvlan, and sr-iov.The Multi-NIC CNI automatically discovers all available secondary interfaces and handles them as a NIC pool. II) Bridging device plugin runtime results and CNI configuration: Multi-NIC CNI can configure CNI of network device in accordance to device plugin allocation results orderly. III) Building-in with several auto-configured CNIs Leveraging advantage point of managing multiple CNIs together with auto-discovery and dynamic interface selection, we built several auto-configured CNIs in the Multi-NIC CNI project including host-interface-local IPAM, multi-config IPAM, multi-gateway routing, and AWS-ipvlan CNI. Check out the project on GitHub \u27a1\ufe0f Multi-NIC CNI . For more insights, please check Medium Blog Post Series KubeCon+CloudNativeCon NA 20222 CNCF-Hosted Co-located Event KubeCon+CloudNativeCon NA 20224 CNCF-Hosted Co-located Event Multi-NIC CNI in Vela IBM Research's AI supercomputer in the cloud The infrastructure powering IBM's Gen AI model development","title":"Multi-NIC CNI"},{"location":"Maintainers/","text":"Maintainers Sunyanan Choochotkaew - sunyanan.choochotkaew1@ibm.com Tatsuhiro Chiba - chiba@jp.ibm.com Min Zhang - minzhang@redhat.com Eran Ben Dror - ebendror@redhat.com","title":"Maintainers"},{"location":"Maintainers/#maintainers","text":"Sunyanan Choochotkaew - sunyanan.choochotkaew1@ibm.com Tatsuhiro Chiba - chiba@jp.ibm.com Min Zhang - minzhang@redhat.com Eran Ben Dror - ebendror@redhat.com","title":"Maintainers"},{"location":"concept/","text":"Key Concept The key concept of Multi-NIC CNI is to provide network segmentation and top-up network bandwidth in the containerization system by attaching secondary network interfaces that is linked to different network interfaces on host (NIC) to pod in a simple, adaptive, and scaled manner. The core features of Multi-NIC CNI are: Common secondary network definition : User can manage only one network definition for multiple secondary interfaces with a common CNI main plugin such as ipvlan, macvlan, and sr-iov. Common NAT-bypassing network solution : All secondary NICs on each host can be assigned with non-conflict CIDR and non-conflict L3 routing configuration that can omit an overlay networking overhead. Particularyly, the CNI is built-in with L3 IPVLAN solution composing of the following functionalities. Interface-host-devision CIDR Computation : compute allocating CIDR range for each host and each interface from a single global subnet with the number of bits for hosts and for interface. L3 Host Route Configuration : configure L3 routes (next hop via dev) in host route table according to the computed CIDR. Distributed IP Allocation Management : manage IP allocation/deallocation distributedly via the communication between CNI program and daemon at each host. Policy-based secondary network attachment : Instead of statically set the desired host's master interface name one by one, user can define a policy on attaching multiple secondary network interfaces such as specifying only the number of desired interfaces, filtering only highspeed NICs. The Multi-NIC operator operates over a custom resource named MultiNicNetwork defined by users. This definition will define a Pod global subnet, common network definition (main CNI and IPAM plugin), and attachment policy. After deploying MultiNicNetwork , NetworkAttachmentDefinition with the same name will be automatically configured and created respectively.","title":"Key Concept"},{"location":"concept/#key-concept","text":"The key concept of Multi-NIC CNI is to provide network segmentation and top-up network bandwidth in the containerization system by attaching secondary network interfaces that is linked to different network interfaces on host (NIC) to pod in a simple, adaptive, and scaled manner. The core features of Multi-NIC CNI are: Common secondary network definition : User can manage only one network definition for multiple secondary interfaces with a common CNI main plugin such as ipvlan, macvlan, and sr-iov. Common NAT-bypassing network solution : All secondary NICs on each host can be assigned with non-conflict CIDR and non-conflict L3 routing configuration that can omit an overlay networking overhead. Particularyly, the CNI is built-in with L3 IPVLAN solution composing of the following functionalities. Interface-host-devision CIDR Computation : compute allocating CIDR range for each host and each interface from a single global subnet with the number of bits for hosts and for interface. L3 Host Route Configuration : configure L3 routes (next hop via dev) in host route table according to the computed CIDR. Distributed IP Allocation Management : manage IP allocation/deallocation distributedly via the communication between CNI program and daemon at each host. Policy-based secondary network attachment : Instead of statically set the desired host's master interface name one by one, user can define a policy on attaching multiple secondary network interfaces such as specifying only the number of desired interfaces, filtering only highspeed NICs. The Multi-NIC operator operates over a custom resource named MultiNicNetwork defined by users. This definition will define a Pod global subnet, common network definition (main CNI and IPAM plugin), and attachment policy. After deploying MultiNicNetwork , NetworkAttachmentDefinition with the same name will be automatically configured and created respectively.","title":"Key Concept"},{"location":"concept/cni-plugins/","text":"Supported CNI Plugins The Multi-NIC CNI controller supports a limited set of common CNI plugins by parsing .spec.plugin.args . Below is the list of supported CNIs and their corresponding arguments: CNI Type Supported Arguments ipvlan master , mode , mtu macvlan master , mode , mtu awsipvlan primaryIP , podIP , master , mode , mtu sriov SriovNetworkNodePolicy: resourceName , priority , mtu , numVfs , isRdma , needVhostNet NetworkAttachmentDefinition: vlan , vlanQos , spoofchk , trust , min_tx_rate , max_tx_rate mellanox None To add support for a new CNI plugin, please refer to this example issue . Support must be implemented in the plugin module by adding a corresponding GetConfig function.","title":"Supported CNI Plugins"},{"location":"concept/cni-plugins/#supported-cni-plugins","text":"The Multi-NIC CNI controller supports a limited set of common CNI plugins by parsing .spec.plugin.args . Below is the list of supported CNIs and their corresponding arguments: CNI Type Supported Arguments ipvlan master , mode , mtu macvlan master , mode , mtu awsipvlan primaryIP , podIP , master , mode , mtu sriov SriovNetworkNodePolicy: resourceName , priority , mtu , numVfs , isRdma , needVhostNet NetworkAttachmentDefinition: vlan , vlanQos , spoofchk , trust , min_tx_rate , max_tx_rate mellanox None To add support for a new CNI plugin, please refer to this example issue . Support must be implemented in the plugin module by adding a corresponding GetConfig function.","title":"Supported CNI Plugins"},{"location":"concept/multi-cloud-support/","text":"Multi-Cloud Support The key benefit of Multi-NIC CNI is that it configures multiple network devices for secondary network solutions on a single CNI call from Kubelet which is delegated by Multus. As a result, Multi-NIC CNI has an extensibility to assign IP address based on both static and dynamic network device selection. We separate secondary network solutions on Cloud into three categories based on the network device selection methods: (i) static selection, (ii) dynamic selection performed by Multi-NIC daemon, and (iii) dynamic selection provided by device plugins such as Mellanox OFED device plugin in NVIDIA network operator . Multi-NIC CNI with static network devices By default, Multi-NIC CNI will have Multi-NIC daemon select a set of network devices based on user-defined strategy (MultiNicNetwork Policy). Then, it will generate a new CNI config per each selected device based on user-defined multi-NIC main and IPAM configurations. IPAM can be either multi-nic-ipam or supported single-NIC IPAM such as whereabouts. The generated config is composed of single-NIC main and single-NIC IPAM configurations. If no strategy is set, Multi-NIC CNI will automatically select all devices in the master network list specified by .spec.masterNets in MultiNicNetwork CR. If the master network list is not set by user, it will be automatically filled by the operator with all available network devices. Multi-NIC CNI with network devices dynamically selected by Multi-NIC daemon If selection policy is set, the Multi-NIC daemon will apply dynamically and return a list of selected list to the CNI. For example, if the pod is annotated with cni-args.nics=2 , only two NICs those are available on the nodes will be selected. In addition to as-is main CNI (ipvlan and macvlan), we implement a new main CNI for AWS support called aws-ipvlan . aws-ipvlan connects to EC2 backend to assign and unassign a private IP addresses on ENA network device. aws-ipvlan is available from v1.1.0 . The following table shows the current supported deployment scenarios and corresponding configurations using default network device selection by Multi-NIC daemon (for both static and dynamic). Scenario Multi-NIC main IPAM (generated) Single-NIC main (generated) Single-NIC IPAM On-premise with L2 ipvlan (L2), macvlan whereabouts ipvlan (L2), macvlan whereabouts ipvlan (L2), macvlan multi-nic-ipam ipvlan (L2), macvlan static VPC with L3 (IBM Cloud, Azure) ipvlan (L3) multi-nic-ipam ipvlan (L3) static AWS aws-ipvlan multi-nic-ipam ipvlan (L2), macvlan static Multi-NIC CNI with network devices dynamically provided by device plugin If network devices are previously allocated by the device plugin such as SR-IoV or RDMA shared device plugin according to the network operator policy (NetworkPolicy), the Multi-NIC daemon will be aware of that by the resource annotation k8s.v1.cni.cncf.io/resourceName when perform selection. There are two supported single-NIC main CNI those are sriov and host-device . sriov plugin is used as-is. For host-device , we further implement mellanox for main CNI for extensible supports of NVIDIA network operator and host-device-ipam for IPAM to assign the host IP address to the pod device. The following table shows the current supported deployment scenarios and corresponding configurations which depend on network resource allocation from the device plugin. mellanox and host-device-ipam are available from v1.2.0 . Scenario Multi-NIC main IPAM (generated) Single-NIC main (generated) Single-NIC IPAM On-premise with SR-IoV sriov whereabouts sriov whereabouts sriov multi-nic-ipam sriov static On-premise with Mellanox OFED mellanox whereabouts host-device whereabouts mellanox host-device-ipam host-device static VPC with Mellanox OFED mellanox host-device-ipam host-device static Hybrid secondary network solution Multi-NIC CNI also allows the cluster to have more than one MultiNicNetwork defintitions. For example, the node can define a set of dedicated network devices limited by pciAddresses in NICClusterPolicy for full ability of GDR with mellanox main plugin. At the same time, the node can have another set of network devices limited by masterNets in MultiNicNetwork to be shared between pods without GDR need using ipvlan main plugin.","title":"Multi-Cloud Support"},{"location":"concept/multi-cloud-support/#multi-cloud-support","text":"The key benefit of Multi-NIC CNI is that it configures multiple network devices for secondary network solutions on a single CNI call from Kubelet which is delegated by Multus. As a result, Multi-NIC CNI has an extensibility to assign IP address based on both static and dynamic network device selection. We separate secondary network solutions on Cloud into three categories based on the network device selection methods: (i) static selection, (ii) dynamic selection performed by Multi-NIC daemon, and (iii) dynamic selection provided by device plugins such as Mellanox OFED device plugin in NVIDIA network operator .","title":"Multi-Cloud Support"},{"location":"concept/multi-cloud-support/#multi-nic-cni-with-static-network-devices","text":"By default, Multi-NIC CNI will have Multi-NIC daemon select a set of network devices based on user-defined strategy (MultiNicNetwork Policy). Then, it will generate a new CNI config per each selected device based on user-defined multi-NIC main and IPAM configurations. IPAM can be either multi-nic-ipam or supported single-NIC IPAM such as whereabouts. The generated config is composed of single-NIC main and single-NIC IPAM configurations. If no strategy is set, Multi-NIC CNI will automatically select all devices in the master network list specified by .spec.masterNets in MultiNicNetwork CR. If the master network list is not set by user, it will be automatically filled by the operator with all available network devices.","title":"Multi-NIC CNI with static network devices"},{"location":"concept/multi-cloud-support/#multi-nic-cni-with-network-devices-dynamically-selected-by-multi-nic-daemon","text":"If selection policy is set, the Multi-NIC daemon will apply dynamically and return a list of selected list to the CNI. For example, if the pod is annotated with cni-args.nics=2 , only two NICs those are available on the nodes will be selected. In addition to as-is main CNI (ipvlan and macvlan), we implement a new main CNI for AWS support called aws-ipvlan . aws-ipvlan connects to EC2 backend to assign and unassign a private IP addresses on ENA network device. aws-ipvlan is available from v1.1.0 . The following table shows the current supported deployment scenarios and corresponding configurations using default network device selection by Multi-NIC daemon (for both static and dynamic). Scenario Multi-NIC main IPAM (generated) Single-NIC main (generated) Single-NIC IPAM On-premise with L2 ipvlan (L2), macvlan whereabouts ipvlan (L2), macvlan whereabouts ipvlan (L2), macvlan multi-nic-ipam ipvlan (L2), macvlan static VPC with L3 (IBM Cloud, Azure) ipvlan (L3) multi-nic-ipam ipvlan (L3) static AWS aws-ipvlan multi-nic-ipam ipvlan (L2), macvlan static","title":"Multi-NIC CNI with network devices dynamically selected by Multi-NIC daemon"},{"location":"concept/multi-cloud-support/#multi-nic-cni-with-network-devices-dynamically-provided-by-device-plugin","text":"If network devices are previously allocated by the device plugin such as SR-IoV or RDMA shared device plugin according to the network operator policy (NetworkPolicy), the Multi-NIC daemon will be aware of that by the resource annotation k8s.v1.cni.cncf.io/resourceName when perform selection. There are two supported single-NIC main CNI those are sriov and host-device . sriov plugin is used as-is. For host-device , we further implement mellanox for main CNI for extensible supports of NVIDIA network operator and host-device-ipam for IPAM to assign the host IP address to the pod device. The following table shows the current supported deployment scenarios and corresponding configurations which depend on network resource allocation from the device plugin. mellanox and host-device-ipam are available from v1.2.0 . Scenario Multi-NIC main IPAM (generated) Single-NIC main (generated) Single-NIC IPAM On-premise with SR-IoV sriov whereabouts sriov whereabouts sriov multi-nic-ipam sriov static On-premise with Mellanox OFED mellanox whereabouts host-device whereabouts mellanox host-device-ipam host-device static VPC with Mellanox OFED mellanox host-device-ipam host-device static","title":"Multi-NIC CNI with network devices dynamically provided by device plugin"},{"location":"concept/multi-cloud-support/#hybrid-secondary-network-solution","text":"Multi-NIC CNI also allows the cluster to have more than one MultiNicNetwork defintitions. For example, the node can define a set of dedicated network devices limited by pciAddresses in NICClusterPolicy for full ability of GDR with mellanox main plugin. At the same time, the node can have another set of network devices limited by masterNets in MultiNicNetwork to be shared between pods without GDR need using ipvlan main plugin.","title":"Hybrid secondary network solution"},{"location":"concept/multi-nic-ipam/","text":"Built-in Multi-NIC Network The built-in multi-NIC network is a common NAT-bypassing network solution without underlay infrastructure dependency based on L3 IPVLAN and neighbor routing table. The target is to attach secondary network interface cards at hosts to the container pods and bypass the costly network address translation to efficiently deliver network packets between pods on different hosts. IPVLAN is a software multiplexing tool that exposes Pod packet and Pod IP directly to master interface (NIC) on the host. In most cases, Pod IPs are not routable by the underlay virtual Cloud infrastructure. Configuring a neighbor route entry (L3 routes) on the host will enable communication between endpoints on the different hosts. Multi-NIC CNI computes a specific CIDR range for each interface and each host incrementally from the user-defined global subnet limiting by defined block sizes. In the above example, as the global subnet is 192.168.0.0/16 with 2 interface bits and 6 host bits, the IP addresses assigned to the first master interface (eth1) will start with 192.168.0.x - 192.168.63.x while that assigned to the second one (eth2) will start with 192.168.64.x - 192.168.127.x. The IP addresses assigned to the first master interface in the first host are further specified to the range 192.168.1.0 - 192.168.1.255. The first and the last addresses are reserved for network address and broadcast address to be managed later. With this blocking range of IP, the L3 routes on each host and interface can be configured without conflict. Common NAT-bypassing network solution The built-in Multi-NIC IPAM and L3 route auto-configuration performs by collaboration of the controller with Multi-NIC CNI and Multi-NIC daemon . The CNI uses orchrestrator storage to keep data synchronize by defining new three custom resources: NAME APIVERSION NAMESPACED KIND cidrs multinic.fms.io/v1 false CIDR hostinterfaces multinic.fms.io/v1 false HostInterface ippools multinic.fms.io/v1 false IPPool CIDR for recording the computed CIDRs. This resource is created or updated when MultiNicNetwork is created or updated with IPAM type multi-nic-ipam . HostInterface is updated HostInterface for keeping discoveried host interface data. This resource is updated if there is a change of host interfaces checked every minute. IPPool for managing IP allocation/deallocation. This resource is created or updated at the same time when CIDR is created or updated. VPC Cluster Requirements **Minimum requirement: ** main plugin support (e.g., kernel version >= 4.2 for ipvlan) L3 mode: enable allowing IP spoofing for each attached interface security group allow target global subnet on secondary subnets allow daemon port (default: 11000) on primary subnet IPAM Configuration In addition to global subnet and designated masterNets , Multi-NIC IPAM requires the following arguments to compute CIDR for each host and each interface. Argument Description Value Remarks vlanMode mode for creating ipvlan l2, l3, l3s For ls3 and l3s mode, the cni will automatically create corresponding host routes in level 3 hostBlock number of address bits for host indexing int (n) the number of assignable host = 2^n interfaceBlock number of address bits for interface indexing int (m) the number of assignable interfaces = 2^m excludeCIDRs list of ip range (CIDR) to exclude list of string example of IPAM-related spec in MultiNicNetwork resource: spec: subnet: \"192.168.0.0/16\" ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 6, \"interfaceBlock\": 2, \"vlanMode\": \"l3\" } multiNICIPAM: true masterNets: - \"10.0.1.0/24\" - \"10.0.2.0/24\" Workflows Interface Discovery CIDR Generation and L3 Route Auto-configuration / Clean up CIDR Generation The current version of CIDR is based on IPv4 which contains 32 bits. Given, hosts=[\"Host1\", \"Host2\"] subnet=192.168.0.0/16 hostBlock=6 interfaceBlock=2 masterNets=[\"10.0.1.0/24\", \"10.0.2.0/24\"] Host1 and Host2 are assigned with index 0 and 1 respectively and, at the same time, interfaces with 10.0.1.0/24 and with 10.0.2.0/24 are assigned with index 0 and 1 respectively. The first 16 bits are reserved for global subnet. The next 2 bits are reserved for interface index. The next 6 bits are reserved for host index. The rest 8 bits are for pod-specific IP address. Accordingly, the pod CIDR for Host1 with network addresss 10.0.1.0/24 is 192.168.0.0/24. (00|000000 from bit 17 to 24) the pod CIDR for Host1 with network addresss 10.0.2.0/24 is 192.168.64.0/24. (01|000000 from bit 17 to 24) the pod CIDR for Host2 with network addresss 10.0.1.0/24 is 192.168.1.0/24. (00|000001 from bit 17 to 24) the pod CIDR for Host2 with network addresss 10.0.2.0/24 is 192.168.65.0/24. (01|000001 from bit 17 to 24) L3 Route Auto-configuration If the vlanMode is set to l3 or l3s, the CNI will configure route table on Pod and Host when the CIDR resource is created as follows. On Pod, the vlan CIDR of each interface is set up to interface block. On Host, the next-hop route is set up to host block. For example, routes configured regarding the above example, # On Pod index 1 at Host1 (IPs = 192.168.0.1, 192.168.64.1) > ip route 192.168.0.0/18 dev net1-0 proto kernel scope link src 192.168.0.1 192.168.64.0/18 dev net1-1 proto kernel scope link src 192.168.64.1 # On Host1 (IPs = 10.0.1.1, 10.0.2.1) when Host2 has IPs 10.0.1.2, 10.0.2.2, > ip route 192.168.1.0/24 via 10.0.1.2 dev eth1 192.168.65.0/24 via 10.0.2.2 dev eth2 IP Allocation / Deallocation The CNI will send a request to daemon running on the deployed host to get a set of IP addresses regarding a set of the interface names. This is a locked operation within the daemon function to prevent allocating the same IP address to different pods at the same time. Host/Interface Block Definition Since the current supported IP is v4 with 32 bits, size of allocatable pods in a single host is limited the subnet block,interface block, and host block as example below. Subnet CIDR (Addresss/Block) Interface Block (Max #Interface) Host Block (Max #Host) Pod/host Size Use case 192.168.0.0/16 2 (4) 6 (64) 256 Small-size application 192.168.0.0/16 2 (4) 8 (256) 64 Medium-size application 192.168.0.0/16 2 (4) 9 (512) 32 Large-size application 192.168.0.0/16 2 (4) 10 (1024) 16 Large-size application The common number of interface is 4 (block=2). The default limitation by hypervisor is 8 (block=3). The current limitation on bare metal is 32 (block=5). A network attachment definition can be defined per application group . However, Each cannot connect to across different application group.","title":"Built-in Multi-NIC Network"},{"location":"concept/multi-nic-ipam/#built-in-multi-nic-network","text":"The built-in multi-NIC network is a common NAT-bypassing network solution without underlay infrastructure dependency based on L3 IPVLAN and neighbor routing table. The target is to attach secondary network interface cards at hosts to the container pods and bypass the costly network address translation to efficiently deliver network packets between pods on different hosts. IPVLAN is a software multiplexing tool that exposes Pod packet and Pod IP directly to master interface (NIC) on the host. In most cases, Pod IPs are not routable by the underlay virtual Cloud infrastructure. Configuring a neighbor route entry (L3 routes) on the host will enable communication between endpoints on the different hosts. Multi-NIC CNI computes a specific CIDR range for each interface and each host incrementally from the user-defined global subnet limiting by defined block sizes. In the above example, as the global subnet is 192.168.0.0/16 with 2 interface bits and 6 host bits, the IP addresses assigned to the first master interface (eth1) will start with 192.168.0.x - 192.168.63.x while that assigned to the second one (eth2) will start with 192.168.64.x - 192.168.127.x. The IP addresses assigned to the first master interface in the first host are further specified to the range 192.168.1.0 - 192.168.1.255. The first and the last addresses are reserved for network address and broadcast address to be managed later. With this blocking range of IP, the L3 routes on each host and interface can be configured without conflict.","title":"Built-in Multi-NIC Network"},{"location":"concept/multi-nic-ipam/#common-nat-bypassing-network-solution","text":"The built-in Multi-NIC IPAM and L3 route auto-configuration performs by collaboration of the controller with Multi-NIC CNI and Multi-NIC daemon . The CNI uses orchrestrator storage to keep data synchronize by defining new three custom resources: NAME APIVERSION NAMESPACED KIND cidrs multinic.fms.io/v1 false CIDR hostinterfaces multinic.fms.io/v1 false HostInterface ippools multinic.fms.io/v1 false IPPool CIDR for recording the computed CIDRs. This resource is created or updated when MultiNicNetwork is created or updated with IPAM type multi-nic-ipam . HostInterface is updated HostInterface for keeping discoveried host interface data. This resource is updated if there is a change of host interfaces checked every minute. IPPool for managing IP allocation/deallocation. This resource is created or updated at the same time when CIDR is created or updated.","title":"Common NAT-bypassing network solution"},{"location":"concept/multi-nic-ipam/#vpc-cluster-requirements","text":"**Minimum requirement: ** main plugin support (e.g., kernel version >= 4.2 for ipvlan) L3 mode: enable allowing IP spoofing for each attached interface security group allow target global subnet on secondary subnets allow daemon port (default: 11000) on primary subnet","title":"VPC Cluster Requirements"},{"location":"concept/multi-nic-ipam/#ipam-configuration","text":"In addition to global subnet and designated masterNets , Multi-NIC IPAM requires the following arguments to compute CIDR for each host and each interface. Argument Description Value Remarks vlanMode mode for creating ipvlan l2, l3, l3s For ls3 and l3s mode, the cni will automatically create corresponding host routes in level 3 hostBlock number of address bits for host indexing int (n) the number of assignable host = 2^n interfaceBlock number of address bits for interface indexing int (m) the number of assignable interfaces = 2^m excludeCIDRs list of ip range (CIDR) to exclude list of string example of IPAM-related spec in MultiNicNetwork resource: spec: subnet: \"192.168.0.0/16\" ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 6, \"interfaceBlock\": 2, \"vlanMode\": \"l3\" } multiNICIPAM: true masterNets: - \"10.0.1.0/24\" - \"10.0.2.0/24\"","title":"IPAM Configuration"},{"location":"concept/multi-nic-ipam/#workflows","text":"","title":"Workflows"},{"location":"concept/multi-nic-ipam/#interface-discovery","text":"","title":"Interface Discovery"},{"location":"concept/multi-nic-ipam/#cidr-generation-and-l3-route-auto-configuration-clean-up","text":"CIDR Generation The current version of CIDR is based on IPv4 which contains 32 bits. Given, hosts=[\"Host1\", \"Host2\"] subnet=192.168.0.0/16 hostBlock=6 interfaceBlock=2 masterNets=[\"10.0.1.0/24\", \"10.0.2.0/24\"] Host1 and Host2 are assigned with index 0 and 1 respectively and, at the same time, interfaces with 10.0.1.0/24 and with 10.0.2.0/24 are assigned with index 0 and 1 respectively. The first 16 bits are reserved for global subnet. The next 2 bits are reserved for interface index. The next 6 bits are reserved for host index. The rest 8 bits are for pod-specific IP address. Accordingly, the pod CIDR for Host1 with network addresss 10.0.1.0/24 is 192.168.0.0/24. (00|000000 from bit 17 to 24) the pod CIDR for Host1 with network addresss 10.0.2.0/24 is 192.168.64.0/24. (01|000000 from bit 17 to 24) the pod CIDR for Host2 with network addresss 10.0.1.0/24 is 192.168.1.0/24. (00|000001 from bit 17 to 24) the pod CIDR for Host2 with network addresss 10.0.2.0/24 is 192.168.65.0/24. (01|000001 from bit 17 to 24) L3 Route Auto-configuration If the vlanMode is set to l3 or l3s, the CNI will configure route table on Pod and Host when the CIDR resource is created as follows. On Pod, the vlan CIDR of each interface is set up to interface block. On Host, the next-hop route is set up to host block. For example, routes configured regarding the above example, # On Pod index 1 at Host1 (IPs = 192.168.0.1, 192.168.64.1) > ip route 192.168.0.0/18 dev net1-0 proto kernel scope link src 192.168.0.1 192.168.64.0/18 dev net1-1 proto kernel scope link src 192.168.64.1 # On Host1 (IPs = 10.0.1.1, 10.0.2.1) when Host2 has IPs 10.0.1.2, 10.0.2.2, > ip route 192.168.1.0/24 via 10.0.1.2 dev eth1 192.168.65.0/24 via 10.0.2.2 dev eth2 IP Allocation / Deallocation The CNI will send a request to daemon running on the deployed host to get a set of IP addresses regarding a set of the interface names. This is a locked operation within the daemon function to prevent allocating the same IP address to different pods at the same time. Host/Interface Block Definition Since the current supported IP is v4 with 32 bits, size of allocatable pods in a single host is limited the subnet block,interface block, and host block as example below. Subnet CIDR (Addresss/Block) Interface Block (Max #Interface) Host Block (Max #Host) Pod/host Size Use case 192.168.0.0/16 2 (4) 6 (64) 256 Small-size application 192.168.0.0/16 2 (4) 8 (256) 64 Medium-size application 192.168.0.0/16 2 (4) 9 (512) 32 Large-size application 192.168.0.0/16 2 (4) 10 (1024) 16 Large-size application The common number of interface is 4 (block=2). The default limitation by hypervisor is 8 (block=3). The current limitation on bare metal is 32 (block=5). A network attachment definition can be defined per application group . However, Each cannot connect to across different application group.","title":"CIDR Generation and L3 Route Auto-configuration / Clean up"},{"location":"concept/network-status/","text":"Network Status Reported multinicnetwork status v1.0.2 Field Value Description computeResults netAddress, numOfHosts summary of CIDR computation if multiNICIPAM=true routeStatus Success mode=l3 and all routes are applied WaitForRoutes mode=l3 but the new CIDR is just recomputed and waiting for route update Failed mode=l3 but some route cannot be applied, need attention Unknown mode=l3 but some daemon cannot be connected N/A mode!=l3 lastSyncTime Date Time timestamp at last synchronization of interfaces and CIDR v1.0.3 or later Field Value Description discovery existDaemon, infoAvailable, cidrProcessed results of hostinterface discovery computeResults netAddress, numOfHosts summary of CIDR computation if multiNICIPAM=true configStatus Success network plugin has been configured. WaitForConfig plugin configuration has not completed yet. Failed failed to configure network plugin routeStatus Success mode=l3 and all routes are applied WaitForRoutes mode=l3 but the new CIDR is just recomputed and waiting for route update Failed mode=l3 but some route cannot be applied, need attention Unknown mode=l3 but some daemon cannot be connected N/A mode!=l3 message ConfigError/RouteError error message (if exists) lastSyncTime Date Time timestamp at last synchronization of interfaces and CIDR","title":"Network Status"},{"location":"concept/network-status/#network-status","text":"Reported multinicnetwork status v1.0.2 Field Value Description computeResults netAddress, numOfHosts summary of CIDR computation if multiNICIPAM=true routeStatus Success mode=l3 and all routes are applied WaitForRoutes mode=l3 but the new CIDR is just recomputed and waiting for route update Failed mode=l3 but some route cannot be applied, need attention Unknown mode=l3 but some daemon cannot be connected N/A mode!=l3 lastSyncTime Date Time timestamp at last synchronization of interfaces and CIDR v1.0.3 or later Field Value Description discovery existDaemon, infoAvailable, cidrProcessed results of hostinterface discovery computeResults netAddress, numOfHosts summary of CIDR computation if multiNICIPAM=true configStatus Success network plugin has been configured. WaitForConfig plugin configuration has not completed yet. Failed failed to configure network plugin routeStatus Success mode=l3 and all routes are applied WaitForRoutes mode=l3 but the new CIDR is just recomputed and waiting for route update Failed mode=l3 but some route cannot be applied, need attention Unknown mode=l3 but some daemon cannot be connected N/A mode!=l3 message ConfigError/RouteError error message (if exists) lastSyncTime Date Time timestamp at last synchronization of interfaces and CIDR","title":"Network Status"},{"location":"concept/policy/","text":"Policy-based secondary network attachment To apply attachment policy, the key attachPolicy need to be specified in MultiNicNetwork and specific arguments can be added specific to Pod annotation (if needed). # MultiNicNetwork spec: attachPolicy: strategy: none|costOpt|perfOpt|devClass Policy Description Status none (default) Apply all NICs in the pool implemented costOpt provide target ideal bandwidth with minimum cost based on HostInterface spec and status TODO perfOpt provide target ideal bandwidth with most available NICs set based on HostInterface spec and status TODO devClass give preference for a specific class of NICs based on DeviceClass custom resource implemented topology give priority to NIC based on Numa affnity of GPU allocation implemented Annotation (CNIArgs) Description Status nics fixed number of interfaces (none, DeviceClass strategy) implemented masters fixed interface names (none strategy) implemented target overridden target bandwidth (CostOpt, PerfOpt strategy) TODO class preferred device class (DeviceClass strategy) implemented None Strategy (none) When none strategy is set or no strategy is set, the Multi-NIC daemon will basically attach all secondary interfaces listed in HostInterface custom resource to the Pod. # MultiNicNetwork spec: attachPolicy: strategy: none To limit the secondary interfaces, the list of subnets can be specified in .spec.masterNets . # MultiNicNetwork spec: masterNets: - 10.0.0.0/16 - 10.1.0.0/16 However, pod can be further annotated to apply only a subset of secondary interfaces with a specific number or name list. For example, - attach only one secondary interface # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"nics\": 1 } }] attach with the secondary interface name eth1 # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"master\": [eth1] } }] If both arguments (nics and master) are applied at the same time, the master argument will be applied. DeviceClass Strategy (devClass) When devClass strategy is set, the Multi-NIC daemon will be additionally aware of class argument specifed in the pod annotation as a filter. # MultiNicNetwork spec: attachPolicy: strategy: devClass # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"class\": \"highspeed\" \"nics\": 1 } }] With the above annotation, one secondary interface that falls into highspeed class defined by DeviceClass will be attached to the Pod. The DeviceClass resource is composed of a list of vendor and product identifiers as below example. # DeviceClass example apiVersion: multinic.fms.io/v1 kind: DeviceClass metadata: name: highspeed spec: ids: - vendor: \"15b3\" products: - \"1019\" - vendor: \"1d0f\" products: - \"efa0\" - \"efa1\" Topology Strategy When topology strategy is set and the number of NICs to select is set lower than availability, Multi-NIC daemon will prioritize the network device by the weight of NUMA where it is located. # MultiNicNetwork spec: attachPolicy: strategy: topology # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"nics\": 1 } }] Weight is the number of GPU devices located on the NUMA that is assigned to the pod by the nvidia device plugin. If no topology file is provided in /var/run/nvidia-topologyd/virtualTopology.xml , the daemon will parse the topology from /sys/devices .","title":"Policy-based secondary network attachment"},{"location":"concept/policy/#policy-based-secondary-network-attachment","text":"To apply attachment policy, the key attachPolicy need to be specified in MultiNicNetwork and specific arguments can be added specific to Pod annotation (if needed). # MultiNicNetwork spec: attachPolicy: strategy: none|costOpt|perfOpt|devClass Policy Description Status none (default) Apply all NICs in the pool implemented costOpt provide target ideal bandwidth with minimum cost based on HostInterface spec and status TODO perfOpt provide target ideal bandwidth with most available NICs set based on HostInterface spec and status TODO devClass give preference for a specific class of NICs based on DeviceClass custom resource implemented topology give priority to NIC based on Numa affnity of GPU allocation implemented Annotation (CNIArgs) Description Status nics fixed number of interfaces (none, DeviceClass strategy) implemented masters fixed interface names (none strategy) implemented target overridden target bandwidth (CostOpt, PerfOpt strategy) TODO class preferred device class (DeviceClass strategy) implemented","title":"Policy-based secondary network attachment"},{"location":"concept/policy/#none-strategy-none","text":"When none strategy is set or no strategy is set, the Multi-NIC daemon will basically attach all secondary interfaces listed in HostInterface custom resource to the Pod. # MultiNicNetwork spec: attachPolicy: strategy: none To limit the secondary interfaces, the list of subnets can be specified in .spec.masterNets . # MultiNicNetwork spec: masterNets: - 10.0.0.0/16 - 10.1.0.0/16 However, pod can be further annotated to apply only a subset of secondary interfaces with a specific number or name list. For example, - attach only one secondary interface # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"nics\": 1 } }] attach with the secondary interface name eth1 # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"master\": [eth1] } }] If both arguments (nics and master) are applied at the same time, the master argument will be applied.","title":"None Strategy (none)"},{"location":"concept/policy/#deviceclass-strategy-devclass","text":"When devClass strategy is set, the Multi-NIC daemon will be additionally aware of class argument specifed in the pod annotation as a filter. # MultiNicNetwork spec: attachPolicy: strategy: devClass # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"class\": \"highspeed\" \"nics\": 1 } }] With the above annotation, one secondary interface that falls into highspeed class defined by DeviceClass will be attached to the Pod. The DeviceClass resource is composed of a list of vendor and product identifiers as below example. # DeviceClass example apiVersion: multinic.fms.io/v1 kind: DeviceClass metadata: name: highspeed spec: ids: - vendor: \"15b3\" products: - \"1019\" - vendor: \"1d0f\" products: - \"efa0\" - \"efa1\"","title":"DeviceClass Strategy (devClass)"},{"location":"concept/policy/#topology-strategy","text":"When topology strategy is set and the number of NICs to select is set lower than availability, Multi-NIC daemon will prioritize the network device by the weight of NUMA where it is located. # MultiNicNetwork spec: attachPolicy: strategy: topology # Pod metadata: annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multi-nic-sample\", \"cni-args\": { \"nics\": 1 } }] Weight is the number of GPU devices located on the NUMA that is assigned to the pod by the nvidia device plugin. If no topology file is provided in /var/run/nvidia-topologyd/virtualTopology.xml , the daemon will parse the topology from /sys/devices .","title":"Topology Strategy"},{"location":"contributing/","text":"Contributor Guide Onboarding Multi-NIC CNI Learn about Multi-NIC CNI. Recommend the blog post Multi-NIC CNI Operator 101: Deep Dive into Container Multi-NIC Simplification . Deep dive into Multi-NIC CNI architecture and communication flow . Prepare Cluster with required environments. Check the requirements . Install Multi-NIC CNI with common MultiNicNetwork definition. Check installation guide . Test network connection. Check user guide . Check development guide . Contributing In General Our project welcomes external contributions. If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us. Note: We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in backlog for a long time, or cannot be accepted at all! Proposing new features If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you wasting your valuable time working on a feature that the project developers are not interested in accepting into the code base. Fixing bugs If you would like to fix a bug, please raise an issue before sending a pull request so it can be tracked. Merge approval The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. For a list of the maintainers, see the MAINTAINERS.md page. Legal Each source file must include a license header for the Apache Software License 2.0. Using the SPDX format is the simplest approach. e.g. /* Copyright <holder> All Rights Reserved. SPDX-License-Identifier: Apache-2.0 */ We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO) - that the Linux\u00ae Kernel community uses to manage code contributions. We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message. Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO: Signed-off-by: John Doe <john.doe@example.com> You can include this automatically when you commit a change to your local git repository using the following command: git commit -s Setup Clone the repo and enter the workspace git clone https://github.com/foundation-model-stack/multi-nic-cni.git cd multi-nic-cni-operator Requirements go operator-sdk utility tools environment substitution envsubst ( gettext ) YAML processor yq ( yq ) Testing Unit test make test Local functional test in your local cluster Build and deploy locally Deploy network Check all-to-all connections Test your new feature (if proposing new feature) Coding style guidelines Follow effective GO . Please run make pr before submitting PRs.","title":"Contributor Guide"},{"location":"contributing/#contributor-guide","text":"","title":"Contributor Guide"},{"location":"contributing/#onboarding-multi-nic-cni","text":"Learn about Multi-NIC CNI. Recommend the blog post Multi-NIC CNI Operator 101: Deep Dive into Container Multi-NIC Simplification . Deep dive into Multi-NIC CNI architecture and communication flow . Prepare Cluster with required environments. Check the requirements . Install Multi-NIC CNI with common MultiNicNetwork definition. Check installation guide . Test network connection. Check user guide . Check development guide .","title":"Onboarding Multi-NIC CNI"},{"location":"contributing/#contributing-in-general","text":"Our project welcomes external contributions. If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us. Note: We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in backlog for a long time, or cannot be accepted at all!","title":"Contributing In General"},{"location":"contributing/#proposing-new-features","text":"If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you wasting your valuable time working on a feature that the project developers are not interested in accepting into the code base.","title":"Proposing new features"},{"location":"contributing/#fixing-bugs","text":"If you would like to fix a bug, please raise an issue before sending a pull request so it can be tracked.","title":"Fixing bugs"},{"location":"contributing/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. For a list of the maintainers, see the MAINTAINERS.md page.","title":"Merge approval"},{"location":"contributing/#legal","text":"Each source file must include a license header for the Apache Software License 2.0. Using the SPDX format is the simplest approach. e.g. /* Copyright <holder> All Rights Reserved. SPDX-License-Identifier: Apache-2.0 */ We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO) - that the Linux\u00ae Kernel community uses to manage code contributions. We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message. Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO: Signed-off-by: John Doe <john.doe@example.com> You can include this automatically when you commit a change to your local git repository using the following command: git commit -s","title":"Legal"},{"location":"contributing/#setup","text":"","title":"Setup"},{"location":"contributing/#clone-the-repo-and-enter-the-workspace","text":"git clone https://github.com/foundation-model-stack/multi-nic-cni.git cd multi-nic-cni-operator","title":"Clone the repo and enter the workspace"},{"location":"contributing/#requirements","text":"go operator-sdk utility tools environment substitution envsubst ( gettext ) YAML processor yq ( yq )","title":"Requirements"},{"location":"contributing/#testing","text":"Unit test make test Local functional test in your local cluster Build and deploy locally Deploy network Check all-to-all connections Test your new feature (if proposing new feature)","title":"Testing"},{"location":"contributing/#coding-style-guidelines","text":"Follow effective GO . Please run make pr before submitting PRs.","title":"Coding style guidelines"},{"location":"contributing/architecture/","text":"Architecture Multi-NIC CNI operator is composed of three main components: controller, daemon, and CNI. The controller implements Operator SDK to create and run a reconcile loop over the CNI custom resource that is MultiNicNetwork, HostInterfaces, CIDR, and IPPool via kube-apiserver. The controller periodically gets interface information from host networks by calling discovery protocol to the daemon and records in HostInterface resource. The controller creates Multus's NetworkAttachmentDefinition and dependent custom resources of main plugin CNI (e.g., sriovnetworknodepolicies of SR-IOV CNI) from MultiNicNetwork 's spec. The generation of CIDR and IPPool , L3 route configuration, and IP allocation/deallocation can be found in Multi-NIC IPAM Plugin . The CNI component is delegated by Multus CNI. It communicates with daemon to select a set of master interfaces according to the policy if defined. If the built-in IPAM is used, it will request for IPs regarding these selected masters. Otherwise, it will delegate the common IPAM plugin to get IP address for each selected NIC. After getting IP addresses, it will delegate the common main plugin (e.g., ipvlan, macvlan, sriov) to configure each additional interface. Note: In addition to CNI-related resource, controller also run a reconcile loop over the Config custom resource to manage daemon and CNI components","title":"Architecture"},{"location":"contributing/architecture/#architecture","text":"Multi-NIC CNI operator is composed of three main components: controller, daemon, and CNI. The controller implements Operator SDK to create and run a reconcile loop over the CNI custom resource that is MultiNicNetwork, HostInterfaces, CIDR, and IPPool via kube-apiserver. The controller periodically gets interface information from host networks by calling discovery protocol to the daemon and records in HostInterface resource. The controller creates Multus's NetworkAttachmentDefinition and dependent custom resources of main plugin CNI (e.g., sriovnetworknodepolicies of SR-IOV CNI) from MultiNicNetwork 's spec. The generation of CIDR and IPPool , L3 route configuration, and IP allocation/deallocation can be found in Multi-NIC IPAM Plugin . The CNI component is delegated by Multus CNI. It communicates with daemon to select a set of master interfaces according to the policy if defined. If the built-in IPAM is used, it will request for IPs regarding these selected masters. Otherwise, it will delegate the common IPAM plugin to get IP address for each selected NIC. After getting IP addresses, it will delegate the common main plugin (e.g., ipvlan, macvlan, sriov) to configure each additional interface. Note: In addition to CNI-related resource, controller also run a reconcile loop over the Config custom resource to manage daemon and CNI components","title":"Architecture"},{"location":"contributing/local_build_push/","text":"Local Development Build images Build CNI operator Set IMAGE_REGISTRY and VERSION environment to target image repository for operator export IMAGE_REGISTRY=<registry> export VERSION=<version> For private image registry, follow these additional steps to add image-pulling secret Put your secret for pulling operator image ( operator-secret.yaml ) to the secret folder mv operator-secret.yaml config/secret Run script to update relevant kustomization files export OPERATOR_SECRET_NAME=$(cat config/secret/operator-secret.yaml|yq .metadata.name) make operator-secret Build and push operator image go mod tidy make docker-build docker-push Build and push bundle image (optional) make bundle make bundle-build bundle-push To test the bundle, run operator-sdk run bundle ${IMAGE_REGISTRY}/multi-nic-cni-bundle:v${VERSION} Build and use catalog source image (optional) make catalog-build catalog-push To test deploy with customized catalog, deploy the catalogsource resource. apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: multi-nic-cni-operator namespace: openshift-marketplace spec: displayName: Multi-NIC CNI Operator publisher: IBM sourceType: grpc image: # <YOUR_CATALOG_IMAGE> updateStrategy: registryPoll: interval: 10m Then, you can deploy via console or subscription resource. Build CNI daemon Set IMAGE_REGISTRY and VERSION environment to target image repository for daemon export IMAGE_REGISTRY=<registry> export VERSION=<version> For private image registry, follow these additional steps to add image-pulling secret Put your secret for pulling daemon image ( daemon-secret.yaml ) to the secret folder mv daemon-secret.yaml config/secret Run script to update relevant kustomization files export DAEMON_SECRET_NAME=$(cat config/secret/daemon-secret.yaml|yq .metadata.name) make daemon-secret Build and push daemon image # build environment: # Linux systems with netlink library cd daemon go mod tidy make docker-build-push This will also build the cni binary and copy the built binary to daemon component. Test # test golang linter make lint # test controller make test # test daemon make test-daemon Install operator make deploy Uninstall operator make undeploy Documentation update We build the document website using GitHub Page site which is based on mkdocs . The material for building the website is under the folder documentation . To test the updated document locally, please install mkdocs in your local environment and test with mkdocs serve under the folder documentation . Please do not include any changes outside documentation folder and push the PR towards doc branch instead of main branch.","title":"Local Development"},{"location":"contributing/local_build_push/#local-development","text":"","title":"Local Development"},{"location":"contributing/local_build_push/#build-images","text":"","title":"Build images"},{"location":"contributing/local_build_push/#build-cni-operator","text":"Set IMAGE_REGISTRY and VERSION environment to target image repository for operator export IMAGE_REGISTRY=<registry> export VERSION=<version> For private image registry, follow these additional steps to add image-pulling secret Put your secret for pulling operator image ( operator-secret.yaml ) to the secret folder mv operator-secret.yaml config/secret Run script to update relevant kustomization files export OPERATOR_SECRET_NAME=$(cat config/secret/operator-secret.yaml|yq .metadata.name) make operator-secret Build and push operator image go mod tidy make docker-build docker-push Build and push bundle image (optional) make bundle make bundle-build bundle-push To test the bundle, run operator-sdk run bundle ${IMAGE_REGISTRY}/multi-nic-cni-bundle:v${VERSION} Build and use catalog source image (optional) make catalog-build catalog-push To test deploy with customized catalog, deploy the catalogsource resource. apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: multi-nic-cni-operator namespace: openshift-marketplace spec: displayName: Multi-NIC CNI Operator publisher: IBM sourceType: grpc image: # <YOUR_CATALOG_IMAGE> updateStrategy: registryPoll: interval: 10m Then, you can deploy via console or subscription resource.","title":"Build CNI operator"},{"location":"contributing/local_build_push/#build-cni-daemon","text":"Set IMAGE_REGISTRY and VERSION environment to target image repository for daemon export IMAGE_REGISTRY=<registry> export VERSION=<version> For private image registry, follow these additional steps to add image-pulling secret Put your secret for pulling daemon image ( daemon-secret.yaml ) to the secret folder mv daemon-secret.yaml config/secret Run script to update relevant kustomization files export DAEMON_SECRET_NAME=$(cat config/secret/daemon-secret.yaml|yq .metadata.name) make daemon-secret Build and push daemon image # build environment: # Linux systems with netlink library cd daemon go mod tidy make docker-build-push This will also build the cni binary and copy the built binary to daemon component.","title":"Build CNI daemon"},{"location":"contributing/local_build_push/#test","text":"# test golang linter make lint # test controller make test # test daemon make test-daemon","title":"Test"},{"location":"contributing/local_build_push/#install-operator","text":"make deploy","title":"Install operator"},{"location":"contributing/local_build_push/#uninstall-operator","text":"make undeploy","title":"Uninstall operator"},{"location":"contributing/local_build_push/#documentation-update","text":"We build the document website using GitHub Page site which is based on mkdocs . The material for building the website is under the folder documentation . To test the updated document locally, please install mkdocs in your local environment and test with mkdocs serve under the folder documentation . Please do not include any changes outside documentation folder and push the PR towards doc branch instead of main branch.","title":"Documentation update"},{"location":"contributing/maintainer/","text":"Upstream Development Release Steps Make sure that doc branch is synced to main branch by pushing PR from doc to main . Make sure that all workflows on the main branch are completed successfully. Create a release on GitHub repository: https://github.com/foundation-model-stack/multi-nic-cni/releases Create a new tag with format release-vX.Y.Z from the main branch. Add auto-generate release note . Insert summarized updates from milestone . Prepare kbuilder image for new version. Dispatch the workflow with image version X.Y.Z . Set new version with the following command and push a PR upgrade version: X.Y.Z to the main branch. make set_version X.Y.Z Update catalog template file and push PR to community operator hub: https://github.com/redhat-openshift-ecosystem/community-operators-prod https://github.com/k8s-operatorhub/community-operators Once the above PR merged, update release page in documentation. Check documentation update guide .","title":"Upstream Development"},{"location":"contributing/maintainer/#upstream-development","text":"","title":"Upstream Development"},{"location":"contributing/maintainer/#release-steps","text":"Make sure that doc branch is synced to main branch by pushing PR from doc to main . Make sure that all workflows on the main branch are completed successfully. Create a release on GitHub repository: https://github.com/foundation-model-stack/multi-nic-cni/releases Create a new tag with format release-vX.Y.Z from the main branch. Add auto-generate release note . Insert summarized updates from milestone . Prepare kbuilder image for new version. Dispatch the workflow with image version X.Y.Z . Set new version with the following command and push a PR upgrade version: X.Y.Z to the main branch. make set_version X.Y.Z Update catalog template file and push PR to community operator hub: https://github.com/redhat-openshift-ecosystem/community-operators-prod https://github.com/k8s-operatorhub/community-operators Once the above PR merged, update release page in documentation. Check documentation update guide .","title":"Release Steps"},{"location":"release/","text":"Release Channel Version IPVLAN L2/L3 AWS IPVLAN Mellanox Host Device MACVLAN SR-IOV VF IP-less (zero host block) Multi-config IPAM stable (default) v1.0.5 \u2713 X X X X X X v1.2.4 v1.2.5 v1.2.6 \u2713 \u2713 \u2713 X X X X beta v1.2.7 \u2713 \u2713 \u2713 \u2713 X X X alpha v1.2.8 \u2713 \u2713 \u2713 \u2713 X X X v1.2.9 \u2713 \u2713 \u2713 \u2713 \u2713 X X v1.3.0 \u2713 \u2713 \u2713 X X \u2713 \u2713","title":"Release"},{"location":"release/#release","text":"Channel Version IPVLAN L2/L3 AWS IPVLAN Mellanox Host Device MACVLAN SR-IOV VF IP-less (zero host block) Multi-config IPAM stable (default) v1.0.5 \u2713 X X X X X X v1.2.4 v1.2.5 v1.2.6 \u2713 \u2713 \u2713 X X X X beta v1.2.7 \u2713 \u2713 \u2713 \u2713 X X X alpha v1.2.8 \u2713 \u2713 \u2713 \u2713 X X X v1.2.9 \u2713 \u2713 \u2713 \u2713 \u2713 X X v1.3.0 \u2713 \u2713 \u2713 X X \u2713 \u2713","title":"Release"},{"location":"release/alpha/","text":"Alpha Channel v1.2.9 Highlights Update SR-IOV CNI support Add VF to PF matching to support VF network devices Upgrade multi nicd go version to 1.24.1 Maintenance Report multi-nicd test coverage (upgrade ginkgo) Increase multi-nicd test coverage to 74.8% Fix selection strategy is not applied and crd in multi-nicd is unsync #320 v1.2.8 Feature Use owner reference to remove NAD upon removal Use RetryOnConflict for finalizer removal Maintenance code increase test coverage to 70.7% upgrade controller go version 1.24.1 with up-to-date dependency documentation add bare metal infra configuration update infra preparation for aws and ibmcloud operation kbuilder workflow hotfix issue template Fix correct interface name on host-device ADD and prevent tabu prefix fix PSP security settings for connection check v1.3.0 (based on v1.2.2) This version does not include the bug fix for incompability to newer version of network operator in https://github.com/foundation-model-stack/multi-nic-cni/pull/182. Improvements: Make all fields except interfaceName in HostInterface.Interfaces optional Add namespace watcher to add NetworkAttachmentDefinition when new namespace is created Set manager container as default container in controller v1.2.2 (deprecated) This version does not include the bug fix for incompability to newer version of network operator in https://github.com/foundation-model-stack/multi-nic-cni/pull/182. Improvements: Multi-config IPAM ( multi-config ) ipam: | { \"type\": \"multi-config\", \"ipam_type\": \"whereabouts\", \"args\": { \"eth1\": { \"range\": \"192.168.0.0/18\" }, \"eth2\": { \"range\": \"192.168.64.0/18\" } } } Static IP support annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multinic-network\", \"cni-args\": { \"masters\": [\"eth1\", \"eth2\"] }, \"ips\": [ \"192.168.0.1/18\", \"192.168.64.1/18\" ] }] v1.2.1 (deprecated) This version does not include the bug fix for incompability to newer version of network operator in https://github.com/foundation-model-stack/multi-nic-cni/pull/182. Improvements: Unmanaged HostNetworkInterface for IP-less network device zero host block/zero interface block apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multinic-unmanaged spec: ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 0, \"interfaceBlock\": 0, \"vlanMode\": \"l2\" } multiNICIPAM: true plugin: cniVersion: \"0.3.0\" type: ipvlan args: mode: l2 specify static cidr of each host apiVersion: multinic.fms.io/v1 kind: HostInterface metadata: name: node-1 labels: multi-nic-unmanaged: \"true\" spec: hostName: node-1 interfaces: - hostIP: \"\" interfaceName: eth1 netAddress: 192.168.0.0/24 pciAddress: \"\" product: \"\" vendor: \"\" - hostIP: \"\" interfaceName: eth2 netAddress: 192.168.1.0/24 pciAddress: \"\" product: \"\" vendor: \"\" Multi-gateway route configuration support apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multinic-multi-gateway spec: ipam: | { \"type\": \"multi-nic-ipam\", ... \"routes\": [{\"dst\": \"10.0.0.0/24\",\"gw\": \"1.1.1.1\"}, {\"dst\": \"10.0.0.0/24\",\"gw\": \"2.2.2.2\"}] } multiNICIPAM: true The above definition will generate the following route on pod: 10.0.0.0/24 nexthop via 1.1.1.1 nexthop via 2.2.2.2","title":"Alpha Channel"},{"location":"release/alpha/#alpha-channel","text":"","title":"Alpha Channel"},{"location":"release/alpha/#v129","text":"Highlights Update SR-IOV CNI support Add VF to PF matching to support VF network devices Upgrade multi nicd go version to 1.24.1 Maintenance Report multi-nicd test coverage (upgrade ginkgo) Increase multi-nicd test coverage to 74.8% Fix selection strategy is not applied and crd in multi-nicd is unsync #320","title":"v1.2.9"},{"location":"release/alpha/#v128","text":"Feature Use owner reference to remove NAD upon removal Use RetryOnConflict for finalizer removal Maintenance code increase test coverage to 70.7% upgrade controller go version 1.24.1 with up-to-date dependency documentation add bare metal infra configuration update infra preparation for aws and ibmcloud operation kbuilder workflow hotfix issue template Fix correct interface name on host-device ADD and prevent tabu prefix fix PSP security settings for connection check","title":"v1.2.8"},{"location":"release/alpha/#v130-based-on-v122","text":"This version does not include the bug fix for incompability to newer version of network operator in https://github.com/foundation-model-stack/multi-nic-cni/pull/182. Improvements: Make all fields except interfaceName in HostInterface.Interfaces optional Add namespace watcher to add NetworkAttachmentDefinition when new namespace is created Set manager container as default container in controller","title":"v1.3.0 (based on v1.2.2)"},{"location":"release/alpha/#v122-deprecated","text":"This version does not include the bug fix for incompability to newer version of network operator in https://github.com/foundation-model-stack/multi-nic-cni/pull/182. Improvements: Multi-config IPAM ( multi-config ) ipam: | { \"type\": \"multi-config\", \"ipam_type\": \"whereabouts\", \"args\": { \"eth1\": { \"range\": \"192.168.0.0/18\" }, \"eth2\": { \"range\": \"192.168.64.0/18\" } } } Static IP support annotations: k8s.v1.cni.cncf.io/networks: | [{ \"name\": \"multinic-network\", \"cni-args\": { \"masters\": [\"eth1\", \"eth2\"] }, \"ips\": [ \"192.168.0.1/18\", \"192.168.64.1/18\" ] }]","title":"v1.2.2 (deprecated)"},{"location":"release/alpha/#v121-deprecated","text":"This version does not include the bug fix for incompability to newer version of network operator in https://github.com/foundation-model-stack/multi-nic-cni/pull/182. Improvements: Unmanaged HostNetworkInterface for IP-less network device zero host block/zero interface block apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multinic-unmanaged spec: ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 0, \"interfaceBlock\": 0, \"vlanMode\": \"l2\" } multiNICIPAM: true plugin: cniVersion: \"0.3.0\" type: ipvlan args: mode: l2 specify static cidr of each host apiVersion: multinic.fms.io/v1 kind: HostInterface metadata: name: node-1 labels: multi-nic-unmanaged: \"true\" spec: hostName: node-1 interfaces: - hostIP: \"\" interfaceName: eth1 netAddress: 192.168.0.0/24 pciAddress: \"\" product: \"\" vendor: \"\" - hostIP: \"\" interfaceName: eth2 netAddress: 192.168.1.0/24 pciAddress: \"\" product: \"\" vendor: \"\" Multi-gateway route configuration support apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multinic-multi-gateway spec: ipam: | { \"type\": \"multi-nic-ipam\", ... \"routes\": [{\"dst\": \"10.0.0.0/24\",\"gw\": \"1.1.1.1\"}, {\"dst\": \"10.0.0.0/24\",\"gw\": \"2.2.2.2\"}] } multiNICIPAM: true The above definition will generate the following route on pod: 10.0.0.0/24 nexthop via 1.1.1.1 nexthop via 2.2.2.2","title":"v1.2.1 (deprecated)"},{"location":"release/beta/","text":"Beta Channel v1.2.7 Highlights support macvlan plugin refactor code structure (add internal packages) Maintenance upgrade controller test to ginkgo V2 generate measurable controller test coverage results improve controller test coverage to 60% Fix correct sample multinicnetwork for macvlan+whereabouts IPAM handle error from ghw.PCI call v1.2.0 (deprecated) Major feature update: Topology-aware NIC Selection RoCE GDR-support CNI (NVIDIA MOFED operator) - mellanox Host-device CNI support NICClusterPolicy aware apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multinic-mellanox-hostdevice spec: ipam: | { \"type\": \"host-device-ipam\" } multiNICIPAM: false plugin: cniVersion: \"0.3.1\" type: mellanox v1.1.0 (deprecated) Major feature update: Multi-cloud support AWS-support CNI Provide aws-ipvlan working with Multi-NIC IPAM Support using Host subnet for Pod subnet for ENA apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multinic-aws-ipvlan spec: ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 8, \"interfaceBlock\": 2, \"vlanMode\": \"l2\" } multiNICIPAM: true plugin: cniVersion: \"0.3.0\" type: aws-ipvlan args: mode: l2","title":"Beta Channel"},{"location":"release/beta/#beta-channel","text":"","title":"Beta Channel"},{"location":"release/beta/#v127","text":"Highlights support macvlan plugin refactor code structure (add internal packages) Maintenance upgrade controller test to ginkgo V2 generate measurable controller test coverage results improve controller test coverage to 60% Fix correct sample multinicnetwork for macvlan+whereabouts IPAM handle error from ghw.PCI call","title":"v1.2.7"},{"location":"release/beta/#v120-deprecated","text":"Major feature update: Topology-aware NIC Selection RoCE GDR-support CNI (NVIDIA MOFED operator) - mellanox Host-device CNI support NICClusterPolicy aware apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multinic-mellanox-hostdevice spec: ipam: | { \"type\": \"host-device-ipam\" } multiNICIPAM: false plugin: cniVersion: \"0.3.1\" type: mellanox","title":"v1.2.0 (deprecated)"},{"location":"release/beta/#v110-deprecated","text":"Major feature update: Multi-cloud support AWS-support CNI Provide aws-ipvlan working with Multi-NIC IPAM Support using Host subnet for Pod subnet for ENA apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multinic-aws-ipvlan spec: ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 8, \"interfaceBlock\": 2, \"vlanMode\": \"l2\" } multiNICIPAM: true plugin: cniVersion: \"0.3.0\" type: aws-ipvlan args: mode: l2","title":"v1.1.0 (deprecated)"},{"location":"release/stable/","text":"Stable Channel (default) v1.2.6 Highlights upgrade go version controller: GO 1.22 daemon, CNI: GO 1.23 remove kube-rbac-proxy add make set_version target to simplify release steps update concept image, user and contributing guide rewrite the highlighted features and add demo and references Fix sample-concheck make error failed to load netconf: post fail: Post \"http://localhost:11000/select\": EOF v1.2.5 Highlights support multiple resource names defined in NicClusterPolicy for Mellanox Host Device use case remove unnecessary selection policy call when network devices have already selected by the device plugin v1.2.4 Major feature update: - The following attributes of HostInterface is changed to optional. NetAddress string `json:\"netAddress,omitempty\"` HostIP string `json:\"hostIP,omitempty\"` Vendor string `json:\"vendor,omitempty\"` Product string `json:\"product,omitempty\"` PciAddress string `json:\"pciAddress,omitempty\"` Set default container of controller pod to manager . Namespace watcher to watch newly-created namespace and create a NetworkAttachmentDefinition of the existing MultiNicNetwork. Fixes - LastSyncTime is nil. - Invalid resource name with prefix. see: https://github.com/foundation-model-stack/multi-nic-cni/pull/182 v1.0.5 Improvements: allow own namespace and single namespace Bug fixes unavailability of unrelated subnet Issue #117 try getting daemon pod if cache not found when calling interface update Issue #118 continue after net-attach-def create/udpate error in PR #114 v1.0.4 (deprecated) CRD changes: config: add following dynamic config values // type ConfigSpec struct UrgentReconcileSeconds int `json:\"urgentReconcileSeconds,omitempty\"` NormalReconcileMinutes int `json:\"normalReconcileMinutes,omitempty\"` LongReconcileMinutes int `json:\"longReconcileMinutes,omitempty\"` ContextTimeoutMinutes int `json:\"contextTimeoutMinutes,omitempty\"` LogLevel int `json:\"logLevel,omitempty\"` Improvements: apply linter code analysis removing unused function, handling errors allow changing reconciler time and log level on the fly from config.multinic change file strcuture separate unit-test folder move constatnt and shared varaible to vars package allow changing synchronization ticker period (TickerIntervalKey) and maximum size of daemon pod watching queue (MAX_QSIZE) from environment variable change logger time encoder to ISO 8601 support OwnNamespace/SingleNamespace namespace deployment Bug fixes: computeResult is reset at one point after CIDR update with no change potentially hang at scale due to API server never return for large amount of listing continue creating NetworkAttachmentDefinition even if it is failed to create/update NetworkAttachmentDefinition in some namespaces v1.0.3 (deprecated) CRD changes: cidrs: add corresponding IPPool to CIDR spec ( spec.cidr.hosts[*].ippool ) multinicnetwork: add host discovery and processing progress ( status.discovery ) add network config status ( status.configStatus ) config: add toleration spec for e2e integration test and future taint usecases ( spec.daemon.tolerations ) Improvements: separate log level on controller manager Verbose Level Information 1 - critical error (cannot create/update resource by k8s API) - \"Set Config\" key - set up log - config error 2 - significant events/failures of multinicnetwork 3 - significant events/failures of cidr 4 (default) - significant events/failures of hostinterface 5 - significant events/failures of ippools 6 - significant events/failures of route configurations 7 - requeue - get deleted resource - debug pointers (e.g., start point of function call) log CNI message on host main plugin: /var/log/multi-nic-cni.log IPAM plugin: /var/log/multi-nic-ipam.log add CI tests multi-nicd test (daemon and CNI components) end-to-end 200-node scale test on kind cluster using kwok Bug fixes: sequential CIDR update blocking in scale (scale issue) unexpected HostInterface deletion when API server is stressed (scale issue) unexpected interface updates on HostInterface when API server is stressed (scale issue) missing HostInterface at initialization (fault-tolerance issue) v1.0.2 (deprecated) First release (as open source) Core Features: Host-interface auto-discovery Single definition for multiple secondary network attachments Multi-NIC IPAM (CIDR computation, IP allocation/deallocation) for multiple secondary subnets L3 configurations on host neighbour route table corresponding to ipvlan CNI plugin with l3 mode Supplementary Features: NIC selection based on specific requested number or specific interface name list Dynamic CIDR updates when detecting added/removed hosts at creation/deletion of multi-nic daemon (periodically) discovering added/removed secondary interfaces Fault tolerance in scale (tested upto 100 nodes x 2 secondary interfaces) with initial synchronization of CIDR, IPPool after controller restarted periodic synchronization of L3 routes for hosts which were restarted and lost the configuration","title":"Stable Channel (default)"},{"location":"release/stable/#stable-channel-default","text":"","title":"Stable Channel (default)"},{"location":"release/stable/#v126","text":"Highlights upgrade go version controller: GO 1.22 daemon, CNI: GO 1.23 remove kube-rbac-proxy add make set_version target to simplify release steps update concept image, user and contributing guide rewrite the highlighted features and add demo and references Fix sample-concheck make error failed to load netconf: post fail: Post \"http://localhost:11000/select\": EOF","title":"v1.2.6"},{"location":"release/stable/#v125","text":"Highlights support multiple resource names defined in NicClusterPolicy for Mellanox Host Device use case remove unnecessary selection policy call when network devices have already selected by the device plugin","title":"v1.2.5"},{"location":"release/stable/#v124","text":"Major feature update: - The following attributes of HostInterface is changed to optional. NetAddress string `json:\"netAddress,omitempty\"` HostIP string `json:\"hostIP,omitempty\"` Vendor string `json:\"vendor,omitempty\"` Product string `json:\"product,omitempty\"` PciAddress string `json:\"pciAddress,omitempty\"` Set default container of controller pod to manager . Namespace watcher to watch newly-created namespace and create a NetworkAttachmentDefinition of the existing MultiNicNetwork. Fixes - LastSyncTime is nil. - Invalid resource name with prefix. see: https://github.com/foundation-model-stack/multi-nic-cni/pull/182","title":"v1.2.4"},{"location":"release/stable/#v105","text":"Improvements: allow own namespace and single namespace Bug fixes unavailability of unrelated subnet Issue #117 try getting daemon pod if cache not found when calling interface update Issue #118 continue after net-attach-def create/udpate error in PR #114","title":"v1.0.5"},{"location":"release/stable/#v104-deprecated","text":"CRD changes: config: add following dynamic config values // type ConfigSpec struct UrgentReconcileSeconds int `json:\"urgentReconcileSeconds,omitempty\"` NormalReconcileMinutes int `json:\"normalReconcileMinutes,omitempty\"` LongReconcileMinutes int `json:\"longReconcileMinutes,omitempty\"` ContextTimeoutMinutes int `json:\"contextTimeoutMinutes,omitempty\"` LogLevel int `json:\"logLevel,omitempty\"` Improvements: apply linter code analysis removing unused function, handling errors allow changing reconciler time and log level on the fly from config.multinic change file strcuture separate unit-test folder move constatnt and shared varaible to vars package allow changing synchronization ticker period (TickerIntervalKey) and maximum size of daemon pod watching queue (MAX_QSIZE) from environment variable change logger time encoder to ISO 8601 support OwnNamespace/SingleNamespace namespace deployment Bug fixes: computeResult is reset at one point after CIDR update with no change potentially hang at scale due to API server never return for large amount of listing continue creating NetworkAttachmentDefinition even if it is failed to create/update NetworkAttachmentDefinition in some namespaces","title":"v1.0.4 (deprecated)"},{"location":"release/stable/#v103-deprecated","text":"CRD changes: cidrs: add corresponding IPPool to CIDR spec ( spec.cidr.hosts[*].ippool ) multinicnetwork: add host discovery and processing progress ( status.discovery ) add network config status ( status.configStatus ) config: add toleration spec for e2e integration test and future taint usecases ( spec.daemon.tolerations ) Improvements: separate log level on controller manager Verbose Level Information 1 - critical error (cannot create/update resource by k8s API) - \"Set Config\" key - set up log - config error 2 - significant events/failures of multinicnetwork 3 - significant events/failures of cidr 4 (default) - significant events/failures of hostinterface 5 - significant events/failures of ippools 6 - significant events/failures of route configurations 7 - requeue - get deleted resource - debug pointers (e.g., start point of function call) log CNI message on host main plugin: /var/log/multi-nic-cni.log IPAM plugin: /var/log/multi-nic-ipam.log add CI tests multi-nicd test (daemon and CNI components) end-to-end 200-node scale test on kind cluster using kwok Bug fixes: sequential CIDR update blocking in scale (scale issue) unexpected HostInterface deletion when API server is stressed (scale issue) unexpected interface updates on HostInterface when API server is stressed (scale issue) missing HostInterface at initialization (fault-tolerance issue)","title":"v1.0.3 (deprecated)"},{"location":"release/stable/#v102-deprecated","text":"First release (as open source) Core Features: Host-interface auto-discovery Single definition for multiple secondary network attachments Multi-NIC IPAM (CIDR computation, IP allocation/deallocation) for multiple secondary subnets L3 configurations on host neighbour route table corresponding to ipvlan CNI plugin with l3 mode Supplementary Features: NIC selection based on specific requested number or specific interface name list Dynamic CIDR updates when detecting added/removed hosts at creation/deletion of multi-nic daemon (periodically) discovering added/removed secondary interfaces Fault tolerance in scale (tested upto 100 nodes x 2 secondary interfaces) with initial synchronization of CIDR, IPPool after controller restarted periodic synchronization of L3 routes for hosts which were restarted and lost the configuration","title":"v1.0.2 (deprecated)"},{"location":"troubleshooting/health-checker/","text":"Troubleshooting with Multi-NIC CNI Health Checker Service See installation steps and usage . StatusCode Message Potential causes Actions 200 Success - - 400 NetworkNotFound NetworkAttachmentDefinition is not created. Check multinicnetwork CR whether the .spec.namespaces is limited to specific list. If included your namespace, check controller log . 401 PluginNotFound CNI binary file is not available. Check CNI binary file . 500 ConfigFailure Configuration input is not valid. Check NetworkAttachmentDefinition.spec.config comparing with full error message from /status response . 501 PluginNotSupport CNI plugin is not supported in the running environment. Check requirements of CNI . 600 NetNSFailured The agent process faces restriction to open a new network Linux namespace (e.g., privileges, namespace limited) Check full error message from /status response . 601 IPAMFailure IPAM CNI fails to assign an IP Check IPAM CNI log and full error message from /status response . 602 PluginExecFailure (depends on failed CNI) Check CNI log and full error message from /status response . 603 PartialFailure Only some network device is not healthy Identify failed network address from /status response . Check connectivity failure . 700 DaemonConnectionFailure Daemon pod is not running or fails to response. Check if multi-nicd is deployed ). If yes, check multi-nicd log . 999 Unknown No health agent running by taint or checker failed to get response. If agent is normally running, check full error message from /status response . Check full error message # port-forward checker pod on one terminal checker=$(kubectl get po -n multi-nic-cni-operator|grep multi-nic-cni-health-checker|awk '{ print $1 }') kubectl port-forward ${checker} -n multi-nic-cni-operator 8080:8080 # request status on specific node export FAILED_NODE= # failed node name curl \"localhost:8080/status?host=${FAILED_NODE}\"","title":"Troubleshooting with Multi-NIC CNI Health Checker Service"},{"location":"troubleshooting/health-checker/#troubleshooting-with-multi-nic-cni-health-checker-service","text":"See installation steps and usage . StatusCode Message Potential causes Actions 200 Success - - 400 NetworkNotFound NetworkAttachmentDefinition is not created. Check multinicnetwork CR whether the .spec.namespaces is limited to specific list. If included your namespace, check controller log . 401 PluginNotFound CNI binary file is not available. Check CNI binary file . 500 ConfigFailure Configuration input is not valid. Check NetworkAttachmentDefinition.spec.config comparing with full error message from /status response . 501 PluginNotSupport CNI plugin is not supported in the running environment. Check requirements of CNI . 600 NetNSFailured The agent process faces restriction to open a new network Linux namespace (e.g., privileges, namespace limited) Check full error message from /status response . 601 IPAMFailure IPAM CNI fails to assign an IP Check IPAM CNI log and full error message from /status response . 602 PluginExecFailure (depends on failed CNI) Check CNI log and full error message from /status response . 603 PartialFailure Only some network device is not healthy Identify failed network address from /status response . Check connectivity failure . 700 DaemonConnectionFailure Daemon pod is not running or fails to response. Check if multi-nicd is deployed ). If yes, check multi-nicd log . 999 Unknown No health agent running by taint or checker failed to get response. If agent is normally running, check full error message from /status response .","title":"Troubleshooting with Multi-NIC CNI Health Checker Service"},{"location":"troubleshooting/health-checker/#check-full-error-message","text":"# port-forward checker pod on one terminal checker=$(kubectl get po -n multi-nic-cni-operator|grep multi-nic-cni-health-checker|awk '{ print $1 }') kubectl port-forward ${checker} -n multi-nic-cni-operator 8080:8080 # request status on specific node export FAILED_NODE= # failed node name curl \"localhost:8080/status?host=${FAILED_NODE}\"","title":"Check full error message"},{"location":"troubleshooting/troubleshooting/","text":"Manual Troubleshooting (Common Issues) ** Please first confirm feature supports on each multi-nic-cni release version from here . ** Issues Multi-NIC CNI Controller gets OOMKilled HostInterface not created No secondary interfaces in HostInterface Pod failed to start Pod failed to start (Summary Table) Ping failed TCP/UDP communication failed. Actions Controller configuration Daemon configuration List in-use pods Get CNI log (available after v1.0.3) Get Controller log Get multi-nicd log Deploy multi-nicd config Set security groups Add secondary interfaces Restart controller Restart multi-nicd Check host secondary interfaces Update daemon pod to use latest version Update controller to use latest version Safe upgrade Multi-NIC CNI operator Customize Multi-NIC CNI controller of operator Issues There are commonly three steps of issue: at pod creation, simple ICMP (ping) communication, TCP/UDP communication. The most complicated one is at pod creation. Before start troubleshooting, set common variables for reference simplicity. export FAILED_POD= # pod that fails to run export FAILED_POD_NAMESPACE= # namespace where the failed pod is supposed to run export FAILED_NODE= # node where pod is deployed export FAILED_NODE_IP = # IP of FAILED_NODE export MULTI_NIC_NAMESPACE= # namespace where multi-nic cni operator is deployed, default=multi-nic-cni-operator Multi-NIC CNI Controller gets OOMKilled This is expected issue in a large cluster where the controller requires large amount of member to operate. Please adjust the resource limit in the controller deployment. For the case of installing via operator hub or operator bundle, please check the step to modify the deployment in Customize Multi-NIC CNI controller of operator . HostInterface not created There are a couple of reasons that the HostInterface is not created. First check the multi-nicd DaemonSet. kubectl get ds multi-nicd -n $MULTI_NIC_NAMESPACE -oyaml daemonsets.apps \"multi-nicd\" not found Check whether no config.multinic.fms.io deployed in the cluster. kubectl get config.multinic multi-nicd -n $MULTI_NIC_NAMESPACE If no config.multinic.fms.io deployed, see Deploy multi-nicd The node has taint that the daemon is not tolerate. kubectl get nodes $FAILED_NODE -o json|jq -r .spec.taints To tolerate the taint , add the tolerate manually to the multi-nicd DaemonSet. kubectl edit $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') Other cases, check controller log No secondary interfaces in HostInterface The HostInterface is created but there is no interface listed in the custom resource. There are two common root causes. Communication between controller and multi-nicd is blocked. Check whether the controller can communicate with multi-nicd: kubectl logs --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE -c manager| grep Join| grep $FAILED_NODE_IP If no line shown up and the full controller log printing Fail to create hostinterface ... cannot update interfaces: Get \"<node IP>/interface\": dial tcp <node IP>:11000: i/o timeout , check set required security group rules Network interfaces are not configured as expected. Check multi-nicd log . If getting cannot list address on <SECONDARY INTERFACE> , please confirm whether IPv4 address on the host. If getting cannot get PCI info: Get \"https://pci-ids.ucw.cz/v2.2/pci.ids.gz\": net/http: TLS handshake timeout , some environment variables need to be set in the config for the multi-nicd container to reach the above address via proxy settings. apiVersion: multinic.fms.io/v1 kind: Config metadata: name: multi-nicd ... spec: ... daemon: env: - name: HTTP_PROXY value: <REPLACE WITH YOUR HTTPS_PROXY> - name: HTTPS_PROXY value: <REPLACE WITH YOUR HTTPS_PROXY> - name: NO_PROXY value: <REPLACE WITH YOUR NO_PROXY> Otherwise, please refer to check interfaces at node's host network . Pod failed to start Issue: Pod stays pending in ContainerCreating status. Get more information from describe kubectl describe $FAILED_POD -n $FAILED_POD_NAMESPACE Find the following keyword from FailedCreatePodSandBox : Network not found CNI binary not found IPAM ExecAdd: failed No available IP address IPAM plugin returned missing IP config zero config Pod failed to start (Summary Table) For those who are familar to action command (e.g., list multinic CRs, list daemon pods), you may troubleshoot with the summary table: Investigate source of issue from top to bottom X refers to no relevance If the issue cannot be solved by configuration (multinicnetwork, annotation, host network, config.multinic) and last patch of controller and multi-nicd , please report the issue with the corresponding log. *The solved bug on CNI binary requires node restart. Potential source of Issue Network not found CNI binary not found - IPAM ExecAdd: failed - IPAM plugin returned missing IP config zero config Fail execPlugin multinicnetwork definition/annotation - annotation missing/mismatch - multinicnetwork wrong configured X - IPAM wrong configured - masters multinicnetwork spec missing (> 1 multinicnetwork) non-IP host: - no master name provided via multi-config or annotation X host network X X X L3: - daemon communication blocked All: - interface missing X controller - net-attach-def not created - daemon not created due to wrong configured (config.multinic) L3: - daemon/hostinterface not created - CIDR/IPPool not created/unsynced X X daemon (multi-nicd) X X L3: - failed to discover hostinterface - IP limit reach All cases: - hang on no-respond API server (should be fixed by #172 ) X X main CNI binary (multi-nic) X X - *failed to clean up previous pod network (should be fixed by #165 ) host-device - *failed to clean up previous pod network (should be fixed by #152 ) X ipam CNI binary (multi-nic-ipam) X X - *failed to clean up previous ip allocation (should be fixed by #104 ) X X 3rd-party CNI binary X - binary missing - 3rd-party IPAM failure X - 3rd-party main plugin failure Network not found kubectl get multinicnetwork # multinicnetwork resource created kubectl get $FAILED_POD -n $FAILED_POD_NAMESPACE -oyaml|grep \"k8s.v1.cni.cncf.io/networks\" # pod annotation matched kubectl get net-attach-def # network-attachment-definition created If net-attach-def is missing ( No resources found in default namespace ), check controller log to see whether the failure comes from misconfiguration in multinicnetwork (Marshal failure) or network-attachment-definition creation request to API server. CNI binary not found The binary file of CNI is not in the expected location read by Multus. The expected location can be found in Multus daemonset as below. kubectl get ds $(kubectl get ds -A\\ |grep multus|head -n 1|awk '{printf \"%s -n %s\", $2, $1}') -ojson\\ |jq .spec.template.spec.volumes Example output: [ ... { \"hostPath\": { \"path\": \"/var/lib/cni/bin\", \"type\": \"\" }, \"name\": \"cnibin\" }, ... ] The expected location is in hostPath of cnibin . missing multi-nic/multi-nic-ipam CNI The CNI directory is probably mounted to a wrong location in the configs.multinic.fms.io CR. Modify mount path ( hostpath attribute ) in spec.daemon.mounts of cnibin to the target location above. kubectl edit config.multinic multi-nicd -n $MULTI_NIC_NAMESPACE missing other CNI such as ipvlan The missing CNI may not be supported. IPAM ExecAdd: failed This error occurs when CNI cannot execute Multi-NIC IPAM which can be caused by multiple reasons as follows. failed to load netconf The configuration cannot be loaded. This is delegated CNI (such as IPVLAN) issue. Find more details from CNI log . \"netx\": address already in use There are a couple of reasons to cause this issue such as IPPool is unsync due to unexpected removal (from operator reinstalltion) or modification of IPPool resource when some assigned pods are still running. IP Address is previously assigned to other pods. This should be handled by this commit . This commit will try assigning the next available address to prevent infinite failure assignment to the same already-in-use IP address. Try updating to latest image of daemon . failed to request ip Response nothing get more information from HostInterface CR: kubectl get HostInterface $FAILED_NODE -oyaml If hostinterfaces.multinic.fms.io \"FAILED_NODE\" not found, check HostInterface is not be created. If no interfaces in .spec.interfaces , check HostInterface does not show the secondary interfaces. Check whether it reaches CIDR block limit, confirm no available IP address Other cases, find more details from multi-nicd log Multi-nicd daemon pod has no response, restart multi-nicd might help. other CNI plugin (such as aws-vpc-cni, sr-iov) failure, check each CNI log. aws-vpc-cni: /host/var/log/aws-routed-eni No available IP address List corresponding Pod CIDR from HostInterface. kubectl get HostInterface $FAILED_NODE -oyaml Check ippools.multinic.fms.io of the corresponding pod CIDR whether the IP address actually reach the limit. If yes, consider changing the host block and interface block in multinicnetworks.multinic.fms.io . IPAM plugin returned missing IP config No IP address set from the multi-nic type IPAM without throwing an error. To troubleshoot, we need additional information from IPAM CNI log . Zero config Zero config occurs when CNI cannot generate configurations from the network-attachment-definition. To troubleshoot, we need additional information from CNI log . Ping failed Issue: Pods cannot ping each other. If the CNI operates at Layer 2 (such as MACVLAN or IPVLAN with L2), please confirm whether the defined Pod CIDR is routable within your cluster. For bare metal cluster which has only a certain VLAN range opened on the switch, please define a VLAN interface instead of the physical NIC on the node. Usually for a bare metal node with a secondary interface, the two ports of NIC2 will be defined as tenant-bond for redundancy, the VLAN interface should be defined following the naming vlanXXX@tenant-bond, where XXX represents a valid open VLAN ID. Please see the following example: 13769: tenant-bond: <BROADCAST,MULTICAST,MASTER,UP,LOWER_UP> mtu 9000 qdisc noqueue state UP group default qlen 1000 link/ether 98:03:9b:8c:55:e4 brd ff:ff:ff:ff:ff:ff 14688: vlan1134@tenant-bond: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc noqueue state UP group default qlen 1000 inet 172.11.3.3/16 brd 172.11.255.255 scope global noprefixroute vlan1134 If the CNI operates at Layer 3, check route status in multinicnetworks.multinic.fms.io . kubectl get multinicnetwork.multinic.fms.io multinic-ipvlanl3 -o json\\ | jq -r .status.routeStatus WaitForRoutes : the new cidr is just recomputed and waiting for route update. Failed : some route cannot be applied, need attention. Check multi-nicd log Unknown : some daemon cannot be connected. N/A : there is no L3 configuration applied. Check whether multinicnetwork.multinic.fms.io is defined with L3 mode and cidrs.multinic.fms.io is created. kubectl get cidrs.multinic.fms.io Success : check set required security group rules TCP/UDP communication failed. Issue: Pods can ping each other but do not get response from TCP/UDP communication such as iPerf. Check whether the multi-nicd detects the other host interfaces. kubectl get po $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') -o json\\ |jq -r .metadata.labels The nubmer in multi-nicd-join should be equal to accumulated number of interfaces from each host in the same zone. Check whether the host secondary interfaces between hosts are connected . If yes, try restarting multi-nic-cni controller node to forcefully synchronize host interfaces. Actions Available configurations on config.multinic/multi-nicd : Controller configuration These following controller configuration values will be applied on-the-fly (no need to restart the controller pod). Configuration Description Default Value .spec.logLevel controller's verbose log level 4 .spec.urgentReconcileSeconds time to requeue reconcile after instant failure in second unit 5 seconds .spec.normalReconcileMinutes time to requeue reconcile while waiting for initial configuration in minute unit 1 minute .spec.longReconcileMinutes time to requeue reconcile when sensing control traffic failure in minute unit 10 minutes .spec.contextTimeoutMinutes time out for API server call context in minute unit 2 minutes Log Levels Verbose Level Information 1 - critical error (cannot create/update resource by k8s API) - \"Set Config\" key - set up log - config error 2 - significant events/failures of multinicnetwork 3 - significant events/failures of cidr 4 (default) - significant events/failures of hostinterface 5 - significant events/failures of ippools 6 - significant events/failures of route configurations 7 - requeue - get deleted resource - debug pointers (e.g., start point of function call) Daemon configuration Configuration Description Type Default Value .spec.daemon.port multi-nicd serving port int 11000 .spec.daemon.mounts additional host-path mount HostPathMount # HostPathMount mounts: - name: mountName podpath: path/on/pod hostpath: path/on/host Additionally, the following common apps/DaemonSet configurations are also available under .spec.daemon . - nodeSelector - image - imagePullSecret - imagePullPolicy - securityContext - env - envFrom - resources - tolerations List in-use pods modify '< MULTINICNETWORK NAME HERE >' in the following command with your target.multinicnetwork name kubectl get po -A -ojson| jq -r '.items[]|select(.metadata.annotations.\"k8s.v1.cni.cncf.io/networks\"==\"< MULTINICNETWORK NAME HERE >\")|.metadata.namespace + \" \" + .metadata.name' Get CNI log (available after v1.0.3) To make CNI log available on the daemon pod, you may mount the the host log path to the daemon pod: Run kubectl edit config.multinic multi-nicd Add the following mount items # config/multi-nicd spec: daemon: mounts: ... - hostpath: /var/log/multi-nic-cni.log name: cni-log podpath: /host/var/log/multi-nic-cni.log - hostpath: /var/log/multi-nic-ipam.log name: ipam-log podpath: /host/var/log/multi-nic-ipam.log # For AWS-IPVLAN main plugin log also add the following lines: # - hostpath: /var/log/multi-nic-aws-ipvlan.log # name: ipam-log # podpath: /host/var/log/multi-nic-aws-ipvlan.log Then, you can get CNI log from the following commands: # default main plugin kubectl exec $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')\\ -- cat /host/var/log/multi-nic-cni.log # multi-nic on aws main plugin kubectl exec $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')\\ -- cat /host/var/log/multi-nic-aws-ipvlan.log # IPAM plugin kubectl exec $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')\\ -- cat /host/var/log/multi-nic-ipam.log Get Controller log kubectl logs --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE -c manager Get multi-nicd log kubectl logs $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') Deploy multi-nicd config Restart the controller pod should create the multi-nicd config automatically. kubectl delete po --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE If not, update the controller to the latest image and restart the controller (recommended). Otherwise, deploy config manually. kubectl create -f https://raw.githubusercontent.com/foundation-model-stack/multi-nic-cni/main/config/samples/config.yaml \\ -n $MULTI_NIC_NAMESPACE Set security groups There are four security group rules that must be opened for Multi-nic CNI. outbound/inbound communication within the same security group outbound/inbound communication of Pod networks inbound multi-nicd serving TCP port (default: 11000) Add secondary interfaces Prepare secondary subnets with required security group rules and enable multiple source IPs from a single vNIC (i.e.g, enable IP spoofing on IBM Cloud) Attach the secondary subnets to instance manual attachment: follows Cloud provider instruction by machine-api-operator: updates an image of machine api controller of the provider to support secondary interface on provider spec. Check example commit in modified controller Restart controller kubectl delete --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE Restart multi-nicd kubectl delete po $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') Check host secondary interfaces Log in to FAILED_NODE with oc debug node/$FAILED_NODE or using nettools with hostNetwork: true . If secondary interfaces do not exist at the host network or an IPv4 address has not been assigned, add the secondary interfaces Update daemon pod to use latest version Check whether the using image set with latest version tag and imagePullPolicy: Always kubectl get daemonset multi-nicd -o yaml -n $MULTI_NIC_NAMESPACE|grep image If not, modify image with the latest version tag and change imagePullPolicy to Always kubectl edit daemonset multi-nicd -n $MULTI_NIC_NAMESPACE Delete the current multi-nicd pods with selector kubectl delete po --selector app=multi-nicd -n $MULTI_NIC_NAMESPACE Check readiness kubectl get po --selector app=multi-nicd -n $MULTI_NIC_NAMESPACE Update controller to use latest version Check whether the using image set with latest version tag and imagePullPolicy: Always kubectl get deploy multi-nic-cni-operator-controller-manager -o yaml -n $MULTI_NIC_NAMESPACE|grep multi-nic-cni-controller -A 2|grep image If not, modify image with the latest version tag and change imagePullPolicy to Always kubectl edit deploy multi-nic-cni-operator-controller-manager -n $MULTI_NIC_NAMESPACE Delete the current multi-nicd pods with selector kubectl delete po --selector control-plane=controller-manager -n $MULTI_NIC_NAMESPACE Check readiness kubectl get po --selector control-plane=controller-manager -n $MULTI_NIC_NAMESPACE Safe upgrade Multi-NIC CNI operator Before bundle version on Operator Hub to v1.0.2 There are three significant changes: Change API group from net.cogadvisor.io to multinic.fms.io . To check API group, kubectl get crd|grep multinicnetworks multinicnetworks.multinic.fms.io 2022-09-27T08:47:35Z Change route configuration logic for handling fault tolerance issue. To check route configuration logic. Run ip rule in any worker host by running oc debug node or using nettools with hostNetwork: true . > ip rule 0: from all lookup local 32765: from 192.168.0.0/16 lookup multinic-ipvlanl3 32766: from all lookup main 32767: from all lookup default If it shows similar rules as above, the route configuration logic is up-to-date. Add multinicnetwork CR to show routeStatus. To check routeStatus key in multinicnetwork CR kubectl get multinicnetwork -o yaml|grep routeStatus routeStatus: Success If all changes are applied (up-to-date) in your current version, there is no need to stop the running workload to reinstall the operator. Check update the daemon pods and update the controller to get the image with latest minor updates and bug fixes. Otherwise, check live migration Customize Multi-NIC CNI controller of operator If the multi-nic-cni operator has been managed by the Operator Lifecycle Manager (olm) (installed by operator-sdk run bundle or via operator hub), the modification to the controller deployment (multi-nic-cni controller pod) will be overriden by the olm. To modify the value such as resource request/limit to the controller pod, you need to edit the .spec.install.spec.deployments section in the ClusterServiceVersion (csv) resource of the multi-nic-cni operator. You can locate the csv resource of multi-nic-cni operator in your cluster from the following command. kubectl get csv -l operators.coreos.com/multi-nic-cni-operator.multi-nic-cni-operator -A Before v1.0.5, the csv are created in all namespaces. You need to edit the csv in the namespace that the controller has been deployed. The modification of csv in the other namespace will not be applied.","title":"Manual Troubleshooting (Common Issues)"},{"location":"troubleshooting/troubleshooting/#manual-troubleshooting-common-issues","text":"** Please first confirm feature supports on each multi-nic-cni release version from here . ** Issues Multi-NIC CNI Controller gets OOMKilled HostInterface not created No secondary interfaces in HostInterface Pod failed to start Pod failed to start (Summary Table) Ping failed TCP/UDP communication failed. Actions Controller configuration Daemon configuration List in-use pods Get CNI log (available after v1.0.3) Get Controller log Get multi-nicd log Deploy multi-nicd config Set security groups Add secondary interfaces Restart controller Restart multi-nicd Check host secondary interfaces Update daemon pod to use latest version Update controller to use latest version Safe upgrade Multi-NIC CNI operator Customize Multi-NIC CNI controller of operator","title":"Manual Troubleshooting (Common Issues)"},{"location":"troubleshooting/troubleshooting/#issues","text":"There are commonly three steps of issue: at pod creation, simple ICMP (ping) communication, TCP/UDP communication. The most complicated one is at pod creation. Before start troubleshooting, set common variables for reference simplicity. export FAILED_POD= # pod that fails to run export FAILED_POD_NAMESPACE= # namespace where the failed pod is supposed to run export FAILED_NODE= # node where pod is deployed export FAILED_NODE_IP = # IP of FAILED_NODE export MULTI_NIC_NAMESPACE= # namespace where multi-nic cni operator is deployed, default=multi-nic-cni-operator","title":"Issues"},{"location":"troubleshooting/troubleshooting/#multi-nic-cni-controller-gets-oomkilled","text":"This is expected issue in a large cluster where the controller requires large amount of member to operate. Please adjust the resource limit in the controller deployment. For the case of installing via operator hub or operator bundle, please check the step to modify the deployment in Customize Multi-NIC CNI controller of operator .","title":"Multi-NIC CNI Controller gets OOMKilled"},{"location":"troubleshooting/troubleshooting/#hostinterface-not-created","text":"There are a couple of reasons that the HostInterface is not created. First check the multi-nicd DaemonSet. kubectl get ds multi-nicd -n $MULTI_NIC_NAMESPACE -oyaml daemonsets.apps \"multi-nicd\" not found Check whether no config.multinic.fms.io deployed in the cluster. kubectl get config.multinic multi-nicd -n $MULTI_NIC_NAMESPACE If no config.multinic.fms.io deployed, see Deploy multi-nicd The node has taint that the daemon is not tolerate. kubectl get nodes $FAILED_NODE -o json|jq -r .spec.taints To tolerate the taint , add the tolerate manually to the multi-nicd DaemonSet. kubectl edit $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') Other cases, check controller log","title":"HostInterface not created"},{"location":"troubleshooting/troubleshooting/#no-secondary-interfaces-in-hostinterface","text":"The HostInterface is created but there is no interface listed in the custom resource. There are two common root causes. Communication between controller and multi-nicd is blocked. Check whether the controller can communicate with multi-nicd: kubectl logs --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE -c manager| grep Join| grep $FAILED_NODE_IP If no line shown up and the full controller log printing Fail to create hostinterface ... cannot update interfaces: Get \"<node IP>/interface\": dial tcp <node IP>:11000: i/o timeout , check set required security group rules Network interfaces are not configured as expected. Check multi-nicd log . If getting cannot list address on <SECONDARY INTERFACE> , please confirm whether IPv4 address on the host. If getting cannot get PCI info: Get \"https://pci-ids.ucw.cz/v2.2/pci.ids.gz\": net/http: TLS handshake timeout , some environment variables need to be set in the config for the multi-nicd container to reach the above address via proxy settings. apiVersion: multinic.fms.io/v1 kind: Config metadata: name: multi-nicd ... spec: ... daemon: env: - name: HTTP_PROXY value: <REPLACE WITH YOUR HTTPS_PROXY> - name: HTTPS_PROXY value: <REPLACE WITH YOUR HTTPS_PROXY> - name: NO_PROXY value: <REPLACE WITH YOUR NO_PROXY> Otherwise, please refer to check interfaces at node's host network .","title":"No secondary interfaces in HostInterface"},{"location":"troubleshooting/troubleshooting/#pod-failed-to-start","text":"Issue: Pod stays pending in ContainerCreating status. Get more information from describe kubectl describe $FAILED_POD -n $FAILED_POD_NAMESPACE Find the following keyword from FailedCreatePodSandBox : Network not found CNI binary not found IPAM ExecAdd: failed No available IP address IPAM plugin returned missing IP config zero config","title":"Pod failed to start"},{"location":"troubleshooting/troubleshooting/#pod-failed-to-start-summary-table","text":"For those who are familar to action command (e.g., list multinic CRs, list daemon pods), you may troubleshoot with the summary table: Investigate source of issue from top to bottom X refers to no relevance If the issue cannot be solved by configuration (multinicnetwork, annotation, host network, config.multinic) and last patch of controller and multi-nicd , please report the issue with the corresponding log. *The solved bug on CNI binary requires node restart. Potential source of Issue Network not found CNI binary not found - IPAM ExecAdd: failed - IPAM plugin returned missing IP config zero config Fail execPlugin multinicnetwork definition/annotation - annotation missing/mismatch - multinicnetwork wrong configured X - IPAM wrong configured - masters multinicnetwork spec missing (> 1 multinicnetwork) non-IP host: - no master name provided via multi-config or annotation X host network X X X L3: - daemon communication blocked All: - interface missing X controller - net-attach-def not created - daemon not created due to wrong configured (config.multinic) L3: - daemon/hostinterface not created - CIDR/IPPool not created/unsynced X X daemon (multi-nicd) X X L3: - failed to discover hostinterface - IP limit reach All cases: - hang on no-respond API server (should be fixed by #172 ) X X main CNI binary (multi-nic) X X - *failed to clean up previous pod network (should be fixed by #165 ) host-device - *failed to clean up previous pod network (should be fixed by #152 ) X ipam CNI binary (multi-nic-ipam) X X - *failed to clean up previous ip allocation (should be fixed by #104 ) X X 3rd-party CNI binary X - binary missing - 3rd-party IPAM failure X - 3rd-party main plugin failure","title":"Pod failed to start (Summary Table)"},{"location":"troubleshooting/troubleshooting/#network-not-found","text":"kubectl get multinicnetwork # multinicnetwork resource created kubectl get $FAILED_POD -n $FAILED_POD_NAMESPACE -oyaml|grep \"k8s.v1.cni.cncf.io/networks\" # pod annotation matched kubectl get net-attach-def # network-attachment-definition created If net-attach-def is missing ( No resources found in default namespace ), check controller log to see whether the failure comes from misconfiguration in multinicnetwork (Marshal failure) or network-attachment-definition creation request to API server.","title":"Network not found"},{"location":"troubleshooting/troubleshooting/#cni-binary-not-found","text":"The binary file of CNI is not in the expected location read by Multus. The expected location can be found in Multus daemonset as below. kubectl get ds $(kubectl get ds -A\\ |grep multus|head -n 1|awk '{printf \"%s -n %s\", $2, $1}') -ojson\\ |jq .spec.template.spec.volumes Example output: [ ... { \"hostPath\": { \"path\": \"/var/lib/cni/bin\", \"type\": \"\" }, \"name\": \"cnibin\" }, ... ] The expected location is in hostPath of cnibin . missing multi-nic/multi-nic-ipam CNI The CNI directory is probably mounted to a wrong location in the configs.multinic.fms.io CR. Modify mount path ( hostpath attribute ) in spec.daemon.mounts of cnibin to the target location above. kubectl edit config.multinic multi-nicd -n $MULTI_NIC_NAMESPACE missing other CNI such as ipvlan The missing CNI may not be supported.","title":"CNI binary not found"},{"location":"troubleshooting/troubleshooting/#ipam-execadd-failed","text":"This error occurs when CNI cannot execute Multi-NIC IPAM which can be caused by multiple reasons as follows. failed to load netconf The configuration cannot be loaded. This is delegated CNI (such as IPVLAN) issue. Find more details from CNI log . \"netx\": address already in use There are a couple of reasons to cause this issue such as IPPool is unsync due to unexpected removal (from operator reinstalltion) or modification of IPPool resource when some assigned pods are still running. IP Address is previously assigned to other pods. This should be handled by this commit . This commit will try assigning the next available address to prevent infinite failure assignment to the same already-in-use IP address. Try updating to latest image of daemon . failed to request ip Response nothing get more information from HostInterface CR: kubectl get HostInterface $FAILED_NODE -oyaml If hostinterfaces.multinic.fms.io \"FAILED_NODE\" not found, check HostInterface is not be created. If no interfaces in .spec.interfaces , check HostInterface does not show the secondary interfaces. Check whether it reaches CIDR block limit, confirm no available IP address Other cases, find more details from multi-nicd log Multi-nicd daemon pod has no response, restart multi-nicd might help. other CNI plugin (such as aws-vpc-cni, sr-iov) failure, check each CNI log. aws-vpc-cni: /host/var/log/aws-routed-eni","title":"IPAM ExecAdd: failed"},{"location":"troubleshooting/troubleshooting/#no-available-ip-address","text":"List corresponding Pod CIDR from HostInterface. kubectl get HostInterface $FAILED_NODE -oyaml Check ippools.multinic.fms.io of the corresponding pod CIDR whether the IP address actually reach the limit. If yes, consider changing the host block and interface block in multinicnetworks.multinic.fms.io .","title":"No available IP address"},{"location":"troubleshooting/troubleshooting/#ipam-plugin-returned-missing-ip-config","text":"No IP address set from the multi-nic type IPAM without throwing an error. To troubleshoot, we need additional information from IPAM CNI log .","title":"IPAM plugin returned missing IP config"},{"location":"troubleshooting/troubleshooting/#zero-config","text":"Zero config occurs when CNI cannot generate configurations from the network-attachment-definition. To troubleshoot, we need additional information from CNI log .","title":"Zero config"},{"location":"troubleshooting/troubleshooting/#ping-failed","text":"Issue: Pods cannot ping each other. If the CNI operates at Layer 2 (such as MACVLAN or IPVLAN with L2), please confirm whether the defined Pod CIDR is routable within your cluster. For bare metal cluster which has only a certain VLAN range opened on the switch, please define a VLAN interface instead of the physical NIC on the node. Usually for a bare metal node with a secondary interface, the two ports of NIC2 will be defined as tenant-bond for redundancy, the VLAN interface should be defined following the naming vlanXXX@tenant-bond, where XXX represents a valid open VLAN ID. Please see the following example: 13769: tenant-bond: <BROADCAST,MULTICAST,MASTER,UP,LOWER_UP> mtu 9000 qdisc noqueue state UP group default qlen 1000 link/ether 98:03:9b:8c:55:e4 brd ff:ff:ff:ff:ff:ff 14688: vlan1134@tenant-bond: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc noqueue state UP group default qlen 1000 inet 172.11.3.3/16 brd 172.11.255.255 scope global noprefixroute vlan1134 If the CNI operates at Layer 3, check route status in multinicnetworks.multinic.fms.io . kubectl get multinicnetwork.multinic.fms.io multinic-ipvlanl3 -o json\\ | jq -r .status.routeStatus WaitForRoutes : the new cidr is just recomputed and waiting for route update. Failed : some route cannot be applied, need attention. Check multi-nicd log Unknown : some daemon cannot be connected. N/A : there is no L3 configuration applied. Check whether multinicnetwork.multinic.fms.io is defined with L3 mode and cidrs.multinic.fms.io is created. kubectl get cidrs.multinic.fms.io Success : check set required security group rules","title":"Ping failed"},{"location":"troubleshooting/troubleshooting/#tcpudp-communication-failed","text":"Issue: Pods can ping each other but do not get response from TCP/UDP communication such as iPerf. Check whether the multi-nicd detects the other host interfaces. kubectl get po $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}') -o json\\ |jq -r .metadata.labels The nubmer in multi-nicd-join should be equal to accumulated number of interfaces from each host in the same zone. Check whether the host secondary interfaces between hosts are connected . If yes, try restarting multi-nic-cni controller node to forcefully synchronize host interfaces.","title":"TCP/UDP communication failed."},{"location":"troubleshooting/troubleshooting/#actions","text":"Available configurations on config.multinic/multi-nicd :","title":"Actions"},{"location":"troubleshooting/troubleshooting/#controller-configuration","text":"These following controller configuration values will be applied on-the-fly (no need to restart the controller pod). Configuration Description Default Value .spec.logLevel controller's verbose log level 4 .spec.urgentReconcileSeconds time to requeue reconcile after instant failure in second unit 5 seconds .spec.normalReconcileMinutes time to requeue reconcile while waiting for initial configuration in minute unit 1 minute .spec.longReconcileMinutes time to requeue reconcile when sensing control traffic failure in minute unit 10 minutes .spec.contextTimeoutMinutes time out for API server call context in minute unit 2 minutes","title":"Controller configuration"},{"location":"troubleshooting/troubleshooting/#log-levels","text":"Verbose Level Information 1 - critical error (cannot create/update resource by k8s API) - \"Set Config\" key - set up log - config error 2 - significant events/failures of multinicnetwork 3 - significant events/failures of cidr 4 (default) - significant events/failures of hostinterface 5 - significant events/failures of ippools 6 - significant events/failures of route configurations 7 - requeue - get deleted resource - debug pointers (e.g., start point of function call)","title":"Log Levels"},{"location":"troubleshooting/troubleshooting/#daemon-configuration","text":"Configuration Description Type Default Value .spec.daemon.port multi-nicd serving port int 11000 .spec.daemon.mounts additional host-path mount HostPathMount # HostPathMount mounts: - name: mountName podpath: path/on/pod hostpath: path/on/host Additionally, the following common apps/DaemonSet configurations are also available under .spec.daemon . - nodeSelector - image - imagePullSecret - imagePullPolicy - securityContext - env - envFrom - resources - tolerations","title":"Daemon configuration"},{"location":"troubleshooting/troubleshooting/#list-in-use-pods","text":"modify '< MULTINICNETWORK NAME HERE >' in the following command with your target.multinicnetwork name kubectl get po -A -ojson| jq -r '.items[]|select(.metadata.annotations.\"k8s.v1.cni.cncf.io/networks\"==\"< MULTINICNETWORK NAME HERE >\")|.metadata.namespace + \" \" + .metadata.name'","title":"List in-use pods"},{"location":"troubleshooting/troubleshooting/#get-cni-log-available-after-v103","text":"To make CNI log available on the daemon pod, you may mount the the host log path to the daemon pod: Run kubectl edit config.multinic multi-nicd Add the following mount items # config/multi-nicd spec: daemon: mounts: ... - hostpath: /var/log/multi-nic-cni.log name: cni-log podpath: /host/var/log/multi-nic-cni.log - hostpath: /var/log/multi-nic-ipam.log name: ipam-log podpath: /host/var/log/multi-nic-ipam.log # For AWS-IPVLAN main plugin log also add the following lines: # - hostpath: /var/log/multi-nic-aws-ipvlan.log # name: ipam-log # podpath: /host/var/log/multi-nic-aws-ipvlan.log Then, you can get CNI log from the following commands: # default main plugin kubectl exec $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')\\ -- cat /host/var/log/multi-nic-cni.log # multi-nic on aws main plugin kubectl exec $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')\\ -- cat /host/var/log/multi-nic-aws-ipvlan.log # IPAM plugin kubectl exec $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')\\ -- cat /host/var/log/multi-nic-ipam.log","title":"Get CNI log (available after v1.0.3)"},{"location":"troubleshooting/troubleshooting/#get-controller-log","text":"kubectl logs --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE -c manager","title":"Get Controller log"},{"location":"troubleshooting/troubleshooting/#get-multi-nicd-log","text":"kubectl logs $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')","title":"Get multi-nicd log"},{"location":"troubleshooting/troubleshooting/#deploy-multi-nicd-config","text":"Restart the controller pod should create the multi-nicd config automatically. kubectl delete po --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE If not, update the controller to the latest image and restart the controller (recommended). Otherwise, deploy config manually. kubectl create -f https://raw.githubusercontent.com/foundation-model-stack/multi-nic-cni/main/config/samples/config.yaml \\ -n $MULTI_NIC_NAMESPACE","title":"Deploy multi-nicd config"},{"location":"troubleshooting/troubleshooting/#set-security-groups","text":"There are four security group rules that must be opened for Multi-nic CNI. outbound/inbound communication within the same security group outbound/inbound communication of Pod networks inbound multi-nicd serving TCP port (default: 11000)","title":"Set security groups"},{"location":"troubleshooting/troubleshooting/#add-secondary-interfaces","text":"Prepare secondary subnets with required security group rules and enable multiple source IPs from a single vNIC (i.e.g, enable IP spoofing on IBM Cloud) Attach the secondary subnets to instance manual attachment: follows Cloud provider instruction by machine-api-operator: updates an image of machine api controller of the provider to support secondary interface on provider spec. Check example commit in modified controller","title":"Add secondary interfaces"},{"location":"troubleshooting/troubleshooting/#restart-controller","text":"kubectl delete --selector control-plane=controller-manager \\ -n $MULTI_NIC_NAMESPACE","title":"Restart controller"},{"location":"troubleshooting/troubleshooting/#restart-multi-nicd","text":"kubectl delete po $(kubectl get po -owide -A|grep multi-nicd\\ |grep $FAILED_NODE|awk '{printf \"%s -n %s\", $2, $1}')","title":"Restart multi-nicd"},{"location":"troubleshooting/troubleshooting/#check-host-secondary-interfaces","text":"Log in to FAILED_NODE with oc debug node/$FAILED_NODE or using nettools with hostNetwork: true . If secondary interfaces do not exist at the host network or an IPv4 address has not been assigned, add the secondary interfaces","title":"Check host secondary interfaces"},{"location":"troubleshooting/troubleshooting/#update-daemon-pod-to-use-latest-version","text":"Check whether the using image set with latest version tag and imagePullPolicy: Always kubectl get daemonset multi-nicd -o yaml -n $MULTI_NIC_NAMESPACE|grep image If not, modify image with the latest version tag and change imagePullPolicy to Always kubectl edit daemonset multi-nicd -n $MULTI_NIC_NAMESPACE Delete the current multi-nicd pods with selector kubectl delete po --selector app=multi-nicd -n $MULTI_NIC_NAMESPACE Check readiness kubectl get po --selector app=multi-nicd -n $MULTI_NIC_NAMESPACE","title":"Update daemon pod to use latest version"},{"location":"troubleshooting/troubleshooting/#update-controller-to-use-latest-version","text":"Check whether the using image set with latest version tag and imagePullPolicy: Always kubectl get deploy multi-nic-cni-operator-controller-manager -o yaml -n $MULTI_NIC_NAMESPACE|grep multi-nic-cni-controller -A 2|grep image If not, modify image with the latest version tag and change imagePullPolicy to Always kubectl edit deploy multi-nic-cni-operator-controller-manager -n $MULTI_NIC_NAMESPACE Delete the current multi-nicd pods with selector kubectl delete po --selector control-plane=controller-manager -n $MULTI_NIC_NAMESPACE Check readiness kubectl get po --selector control-plane=controller-manager -n $MULTI_NIC_NAMESPACE","title":"Update controller to use latest version"},{"location":"troubleshooting/troubleshooting/#safe-upgrade-multi-nic-cni-operator","text":"Before bundle version on Operator Hub to v1.0.2 There are three significant changes: Change API group from net.cogadvisor.io to multinic.fms.io . To check API group, kubectl get crd|grep multinicnetworks multinicnetworks.multinic.fms.io 2022-09-27T08:47:35Z Change route configuration logic for handling fault tolerance issue. To check route configuration logic. Run ip rule in any worker host by running oc debug node or using nettools with hostNetwork: true . > ip rule 0: from all lookup local 32765: from 192.168.0.0/16 lookup multinic-ipvlanl3 32766: from all lookup main 32767: from all lookup default If it shows similar rules as above, the route configuration logic is up-to-date. Add multinicnetwork CR to show routeStatus. To check routeStatus key in multinicnetwork CR kubectl get multinicnetwork -o yaml|grep routeStatus routeStatus: Success If all changes are applied (up-to-date) in your current version, there is no need to stop the running workload to reinstall the operator. Check update the daemon pods and update the controller to get the image with latest minor updates and bug fixes. Otherwise, check live migration","title":"Safe upgrade Multi-NIC CNI operator"},{"location":"troubleshooting/troubleshooting/#customize-multi-nic-cni-controller-of-operator","text":"If the multi-nic-cni operator has been managed by the Operator Lifecycle Manager (olm) (installed by operator-sdk run bundle or via operator hub), the modification to the controller deployment (multi-nic-cni controller pod) will be overriden by the olm. To modify the value such as resource request/limit to the controller pod, you need to edit the .spec.install.spec.deployments section in the ClusterServiceVersion (csv) resource of the multi-nic-cni operator. You can locate the csv resource of multi-nic-cni operator in your cluster from the following command. kubectl get csv -l operators.coreos.com/multi-nic-cni-operator.multi-nic-cni-operator -A Before v1.0.5, the csv are created in all namespaces. You need to edit the csv in the namespace that the controller has been deployed. The modification of csv in the other namespace will not be applied.","title":"Customize Multi-NIC CNI controller of operator"},{"location":"user_guide/","text":"Installation Requirements Secondary interfaces attached to worker nodes, check terraform script here . Secondary interfaces must have an IPv4 address assigned. Multus CNI installation; compatible with networkAttachmentDefinition and pod annotation in multus-cni v3.8 For IPVLAN L3 CNI, the following configurations are additionally required enable allowing IP spoofing for each attached interface set security group to allow IPs in the target container subnet IPVLAN support (kernel version >= 4.2) 1. Install Operator by OperatorHub Kubernetes with OLM: check multi-nic-cni-operator on OperatorHub.io Openshift Container Platform: Search for multi-nic-cni-operator in OperatorHub Recommended to deploy in the same default namespace for health check service , which is multi-nic-cni-operator . (available version >= v1.0.5) by manifests with kubectl kubectl apply -f deploy/ by bundle with operator-sdk operator-sdk run bundle ghcr.io/foundation-model-stack/multi-nic-cni-bundle:v1.0.5 -n multi-nic-cni-operator 2. Check if Cluster is Ready Controller and multi-nicd daemon pods are running annd hostinterface are created. > kubectl get po -n multi-nic-cni-operator NAME READY STATUS RESTARTS AGE multi-nic-cni-operator-controller-manager-6cbf4d57f9-zxx89 1/1 Running 0 40h multi-nicd-bgrvq 1/1 Running 0 40h multi-nicd-c4hdv 1/1 Running 0 40h multi-nicd-f2gkk 1/1 Running 0 40h ... > kubectl get hostinterfaces NAME AGE worker-node-1 40h worker-node-2 40h worker-node-3 40h If multi-nicd is not running or hostinterface is not created, check this troubleshooting guide . Secondary interfaces have been correctly detected. # Please replace the host interface with your worker name. > kubectl get hostinterface worker-node-1 -oyaml apiVersion: multinic.fms.io/v1 kind: HostInterface metadata: name: worker-node-1 ... spec: hostName: worker-node-1 interfaces: - hostIP: 10.0.1.0 interfaceName: eth1 ... If secondary interface is not added, check this troubleshooting guide . 3. Deploy MultiNicNetwork resource Common MultiNicNetwork The following MultiNicNetwork is available for any Cloud infrastructure which meets the requirements . MultiNicNetwork CR # network.yaml apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multi-nic-sample spec: subnet: \"192.168.0.0/16\" ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 6, \"interfaceBlock\": 2, \"vlanMode\": \"l3\" } multiNICIPAM: true plugin: cniVersion: \"0.3.0\" type: ipvlan args: mode: l3 attachPolicy: strategy: none namespaces: - default Argument Description Value Remarks subnet cluster-wide subnet for all hosts and pods CIDR range currently support only v4 hostBlock number of address bits for host indexing int (n) the number of assignable host = 2^n ipam ipam plugin config string ipam can be single-NIC IPAM (e.g., whereabouts, VPC-native IPAM) or multi-NIC IPAM (e.g., Multi-NIC IPAM Plugin ) multiNicIPAM indicator of ipam type bool true if ipam returns multiple IPs from masters key of NetworkAttachmentDefinition config at once, false if ipam returns only single IP from static config in ipam block plugin main plugin config NetConf + plugin-specific arguments see supported CNI plugins for the list of supported CNI plugins and their arguments attachPolicy attachment policy policy strategy with corresponding arguments to select host NICs to be master of secondary interfaces on Pod namespaces list of namespaces to apply the network definitions (i.e., to create NetworkAttachmentDefinition resource) []string apply to all namespace if not specified. new item can be added to the list by kubectl edit to create new NetworkAttachmentDefinition. the created NetworkAttachmentDefinition must be deleted manually if needed. Prepare network.yaml as shown in the example Deploy the network definition. kubectl apply -f network.yaml After deployment, the operator will create NetworkAttachmentDefinition of Multus CNI from MultiNicNetwork as well as dependent resource such as SriovNetworkNodePolicy , SriovNetwork for sriov plugin. Additional MultiNicNetwork for specific Cloud infrastructure In addition to the common MultiNicNetwork with IPVLAN-L3 , Multi-NIC CNI offers unique benefits tailored to each specific cloud infrastructure from v1.1.0, as shown below. Corresponding network options (MultiNicNetwork) for each infrastructure are listed accordingly. Please check latest release . Multi-NIC CNI Features IBM Cloud Bare Metal AWS Azure (tentative) Single definition for multiple attachments - dynamic interface discovery - policy-based NIC selection \u2713 \u2713 \u2713 \u2713 CIDR/IP management \u2713 * * \u2713 L3 Route configuration \u2713 X X \u2713 \u2713: beneficial *: optional (e.g., replacable by whereabout, aws-vpc-cni IPAM) X: non-beneficial as using L2 IBM Cloud, Azure IPVLAN L3 kubectl apply -f config/samples/multinicnetwork/ipvlanl3.yaml Mellanox Host Device with Host Device IPAM kubectl apply -f config/samples/multinicnetwork/mellanox_hostdevice.yaml BareMetal MACVLAN with whereabout IPAM kubectl apply -f config/samples/multinicnetwork/macvlan.yaml MACVLAN with Multi-NIC IPAM kubectl apply -f config/samples/multinicnetwork/macvlan_ipam.yaml IPVLAN L2 with whereabout IPAM kubectl apply -f config/samples/multinicnetwork/ipvlanl2.yaml SR-IoV with Multi-NIC IPAM kubectl apply -f config/samples/multinicnetwork/sriov.yaml Mellanox Host Device with Host Device IPAM kubectl apply -f config/samples/multinicnetwork/mellanox_hostdevice.yaml AWS IPVLAN L2 with AWS-VPC-connecting IPAM kubectl apply -f config/samples/multinicnetwork/awsipvlan.yaml 4. Check if MultiNicNetwork is Ready Get multinicnetwork's status: > kubectl get multinicnetwork -oyaml - apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: ... name: multi-nic-cni-operator-ipvlanl3 spec: ... status: computeResults: - netAddress: 10.241.130.0/24 numOfHosts: 3 configStatus: Success discovery: cidrProcessed: 3 existDaemon: 3 infoAvailable: 3 lastSyncTime: \"2025-03-05T23:49:48Z\" message: \"\" routeStatus: Success The configStatus should be \"Success\". discovery should show the number of available interfaces infoAvailable . If multi-nic-ipam is specified in spec, computeResults should be added. If L3 mode is used with IPVLAN, routeStatus should be \"Success\". 5. Check Connection By Script see check connection . By Health Checker Service Deploy health check and agents to the cluster to serve a functional and connetion checking on-demand and periodically exporting to Prometheus metric server. See more detail . Clean up installed by manifests with kubectl kubectl delete -f deploy/ installed by bundle with operator-sdk operator-sdk cleanup multi-nic-cni-operator -n multi-nic-cni-operator","title":"Installation"},{"location":"user_guide/#installation","text":"","title":"Installation"},{"location":"user_guide/#requirements","text":"Secondary interfaces attached to worker nodes, check terraform script here . Secondary interfaces must have an IPv4 address assigned. Multus CNI installation; compatible with networkAttachmentDefinition and pod annotation in multus-cni v3.8 For IPVLAN L3 CNI, the following configurations are additionally required enable allowing IP spoofing for each attached interface set security group to allow IPs in the target container subnet IPVLAN support (kernel version >= 4.2)","title":"Requirements"},{"location":"user_guide/#1-install-operator","text":"by OperatorHub Kubernetes with OLM: check multi-nic-cni-operator on OperatorHub.io Openshift Container Platform: Search for multi-nic-cni-operator in OperatorHub Recommended to deploy in the same default namespace for health check service , which is multi-nic-cni-operator . (available version >= v1.0.5) by manifests with kubectl kubectl apply -f deploy/ by bundle with operator-sdk operator-sdk run bundle ghcr.io/foundation-model-stack/multi-nic-cni-bundle:v1.0.5 -n multi-nic-cni-operator","title":"1. Install Operator"},{"location":"user_guide/#2-check-if-cluster-is-ready","text":"Controller and multi-nicd daemon pods are running annd hostinterface are created. > kubectl get po -n multi-nic-cni-operator NAME READY STATUS RESTARTS AGE multi-nic-cni-operator-controller-manager-6cbf4d57f9-zxx89 1/1 Running 0 40h multi-nicd-bgrvq 1/1 Running 0 40h multi-nicd-c4hdv 1/1 Running 0 40h multi-nicd-f2gkk 1/1 Running 0 40h ... > kubectl get hostinterfaces NAME AGE worker-node-1 40h worker-node-2 40h worker-node-3 40h If multi-nicd is not running or hostinterface is not created, check this troubleshooting guide . Secondary interfaces have been correctly detected. # Please replace the host interface with your worker name. > kubectl get hostinterface worker-node-1 -oyaml apiVersion: multinic.fms.io/v1 kind: HostInterface metadata: name: worker-node-1 ... spec: hostName: worker-node-1 interfaces: - hostIP: 10.0.1.0 interfaceName: eth1 ... If secondary interface is not added, check this troubleshooting guide .","title":"2. Check if Cluster is Ready"},{"location":"user_guide/#3-deploy-multinicnetwork-resource","text":"","title":"3. Deploy MultiNicNetwork resource"},{"location":"user_guide/#common-multinicnetwork","text":"The following MultiNicNetwork is available for any Cloud infrastructure which meets the requirements . MultiNicNetwork CR # network.yaml apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: name: multi-nic-sample spec: subnet: \"192.168.0.0/16\" ipam: | { \"type\": \"multi-nic-ipam\", \"hostBlock\": 6, \"interfaceBlock\": 2, \"vlanMode\": \"l3\" } multiNICIPAM: true plugin: cniVersion: \"0.3.0\" type: ipvlan args: mode: l3 attachPolicy: strategy: none namespaces: - default Argument Description Value Remarks subnet cluster-wide subnet for all hosts and pods CIDR range currently support only v4 hostBlock number of address bits for host indexing int (n) the number of assignable host = 2^n ipam ipam plugin config string ipam can be single-NIC IPAM (e.g., whereabouts, VPC-native IPAM) or multi-NIC IPAM (e.g., Multi-NIC IPAM Plugin ) multiNicIPAM indicator of ipam type bool true if ipam returns multiple IPs from masters key of NetworkAttachmentDefinition config at once, false if ipam returns only single IP from static config in ipam block plugin main plugin config NetConf + plugin-specific arguments see supported CNI plugins for the list of supported CNI plugins and their arguments attachPolicy attachment policy policy strategy with corresponding arguments to select host NICs to be master of secondary interfaces on Pod namespaces list of namespaces to apply the network definitions (i.e., to create NetworkAttachmentDefinition resource) []string apply to all namespace if not specified. new item can be added to the list by kubectl edit to create new NetworkAttachmentDefinition. the created NetworkAttachmentDefinition must be deleted manually if needed. Prepare network.yaml as shown in the example Deploy the network definition. kubectl apply -f network.yaml After deployment, the operator will create NetworkAttachmentDefinition of Multus CNI from MultiNicNetwork as well as dependent resource such as SriovNetworkNodePolicy , SriovNetwork for sriov plugin.","title":"Common MultiNicNetwork"},{"location":"user_guide/#additional-multinicnetwork-for-specific-cloud-infrastructure","text":"In addition to the common MultiNicNetwork with IPVLAN-L3 , Multi-NIC CNI offers unique benefits tailored to each specific cloud infrastructure from v1.1.0, as shown below. Corresponding network options (MultiNicNetwork) for each infrastructure are listed accordingly. Please check latest release . Multi-NIC CNI Features IBM Cloud Bare Metal AWS Azure (tentative) Single definition for multiple attachments - dynamic interface discovery - policy-based NIC selection \u2713 \u2713 \u2713 \u2713 CIDR/IP management \u2713 * * \u2713 L3 Route configuration \u2713 X X \u2713 \u2713: beneficial *: optional (e.g., replacable by whereabout, aws-vpc-cni IPAM) X: non-beneficial as using L2","title":"Additional MultiNicNetwork for specific Cloud infrastructure"},{"location":"user_guide/#ibm-cloud-azure","text":"IPVLAN L3 kubectl apply -f config/samples/multinicnetwork/ipvlanl3.yaml Mellanox Host Device with Host Device IPAM kubectl apply -f config/samples/multinicnetwork/mellanox_hostdevice.yaml","title":"IBM Cloud, Azure"},{"location":"user_guide/#baremetal","text":"MACVLAN with whereabout IPAM kubectl apply -f config/samples/multinicnetwork/macvlan.yaml MACVLAN with Multi-NIC IPAM kubectl apply -f config/samples/multinicnetwork/macvlan_ipam.yaml IPVLAN L2 with whereabout IPAM kubectl apply -f config/samples/multinicnetwork/ipvlanl2.yaml SR-IoV with Multi-NIC IPAM kubectl apply -f config/samples/multinicnetwork/sriov.yaml Mellanox Host Device with Host Device IPAM kubectl apply -f config/samples/multinicnetwork/mellanox_hostdevice.yaml","title":"BareMetal"},{"location":"user_guide/#aws","text":"IPVLAN L2 with AWS-VPC-connecting IPAM kubectl apply -f config/samples/multinicnetwork/awsipvlan.yaml","title":"AWS"},{"location":"user_guide/#4-check-if-multinicnetwork-is-ready","text":"Get multinicnetwork's status: > kubectl get multinicnetwork -oyaml - apiVersion: multinic.fms.io/v1 kind: MultiNicNetwork metadata: ... name: multi-nic-cni-operator-ipvlanl3 spec: ... status: computeResults: - netAddress: 10.241.130.0/24 numOfHosts: 3 configStatus: Success discovery: cidrProcessed: 3 existDaemon: 3 infoAvailable: 3 lastSyncTime: \"2025-03-05T23:49:48Z\" message: \"\" routeStatus: Success The configStatus should be \"Success\". discovery should show the number of available interfaces infoAvailable . If multi-nic-ipam is specified in spec, computeResults should be added. If L3 mode is used with IPVLAN, routeStatus should be \"Success\".","title":"4. Check if MultiNicNetwork is Ready"},{"location":"user_guide/#5-check-connection","text":"","title":"5. Check Connection"},{"location":"user_guide/#by-script","text":"see check connection .","title":"By Script"},{"location":"user_guide/#by-health-checker-service","text":"Deploy health check and agents to the cluster to serve a functional and connetion checking on-demand and periodically exporting to Prometheus metric server. See more detail .","title":"By Health Checker Service"},{"location":"user_guide/#clean-up","text":"installed by manifests with kubectl kubectl delete -f deploy/ installed by bundle with operator-sdk operator-sdk cleanup multi-nic-cni-operator -n multi-nic-cni-operator","title":"Clean up"},{"location":"user_guide/user/","text":"MultiNicNetwork Usage and Testing Steps 1. check available network > oc get multinicnetwork NAME AGE multi-nic-cni-operator-ipvlanl3 12s 2. annotate the pod metadata: annotations: k8s.v1.cni.cncf.io/networks: multi-nic-sample Check connections One-time peer-to-peer This is a quick test by a randomly picked-up pair. Set target peer export SERVER_HOST_NAME=<target-server-node-name> export CLIENT_HOST_NAME=<target-client-node-name> If the target peer is not set, the last two of listed nodes will be selected as server and client, respectively. Run the test make sample-concheck Example output: # pod/multi-nic-iperf3-server created # pod/multi-nic-iperf3-server condition met # pod/multi-nic-iperf3-client created # pod/multi-nic-iperf3-client condition met # [ 5] local 192.168.0.66 port 46284 connected to 192.168.0.130 port 5201 # [ ID] Interval Transfer Bitrate Retr Cwnd # [ 5] 0.00-1.00 sec 121 MBytes 1.02 Gbits/sec 0 3.04 MBytes # [ 5] 1.00-2.00 sec 114 MBytes 954 Mbits/sec 0 3.04 MBytes # [ 5] 2.00-3.00 sec 115 MBytes 964 Mbits/sec 45 2.19 MBytes # [ 5] 3.00-4.00 sec 114 MBytes 954 Mbits/sec 45 1.67 MBytes # [ 5] 4.00-5.00 sec 114 MBytes 954 Mbits/sec 0 1.77 MBytes # - - - - - - - - - - - - - - - - - - - - - - - - - # [ ID] Interval Transfer Bitrate Retr # [ 5] 0.00-5.00 sec 577 MBytes 969 Mbits/sec 90 sender # [ 5] 0.00-5.04 sec 574 MBytes 956 Mbits/sec receiver # iperf Done. # pod \"multi-nic-iperf3-client\" deleted # pod \"multi-nic-iperf3-server\" deleted One-time all-to-all This is recommended for small cluster ( <10 HostInterfaces ) Run make concheck Example output: # serviceaccount/multi-nic-concheck-account created # clusterrole.rbac.authorization.k8s.io/multi-nic-concheck-cr created # clusterrolebinding.rbac.authorization.k8s.io/multi-nic-concheck-cr-binding created # job.batch/multi-nic-concheck created # Wait for job/multi-nic-concheck to complete # job.batch/multi-nic-concheck condition met # 2023/02/14 01:22:21 Config # W0214 01:22:21.976565 1 client_config.go:617] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. # 2023/02/14 01:22:23 2/2 servers successfully created # 2023/02/14 01:22:23 p-cni-operator-ipvlanl3-multi-nic-n7zf6-worker-2-zt5l5-serv: Pending ... # 2023/02/14 01:23:13 2/2 clients successfully finished # ########################################### # ## Connection Check: multi-nic-cni-operator-ipvlanl3 # ########################################### # FROM TO CONNECTED/TOTAL IPs BANDWIDTHs # multi-nic-n7zf6-worker-2-zt5l5 multi-nic-n7zf6-worker-2-zxw2n 2/2 [192.168.0.65 192.168.64.65] [ 1.05Gbits/sec 1.03Gbits/sec] # multi-nic-n7zf6-worker-2-zxw2n multi-nic-n7zf6-worker-2-zt5l5 2/2 [192.168.0.129 192.168.64.129] [ 934Mbits/sec 937Mbits/sec] # ########################################### # 2023/02/14 01:23:13 multi-nic-cni-operator-ipvlanl3 checked If the job takes longer than 50 minutes (mostly in large cluster), you will get error: timed out waiting for the condition on jobs/multi-nic-concheck Check the test progress directly: kubectl get po -w When the multi-nic-concheck job has completed, check the log: kubectl logs job/multi-nic-concheck If some connection failed, you may investigate the failure from log of the iperf3 client pod. Clean up the job make clean-concheck","title":"MultiNicNetwork Usage and Testing"},{"location":"user_guide/user/#multinicnetwork-usage-and-testing","text":"","title":"MultiNicNetwork Usage and Testing"},{"location":"user_guide/user/#steps","text":"","title":"Steps"},{"location":"user_guide/user/#1-check-available-network","text":"> oc get multinicnetwork NAME AGE multi-nic-cni-operator-ipvlanl3 12s","title":"1. check available network"},{"location":"user_guide/user/#2-annotate-the-pod","text":"metadata: annotations: k8s.v1.cni.cncf.io/networks: multi-nic-sample","title":"2. annotate the pod"},{"location":"user_guide/user/#check-connections","text":"","title":"Check connections"},{"location":"user_guide/user/#one-time-peer-to-peer","text":"This is a quick test by a randomly picked-up pair. Set target peer export SERVER_HOST_NAME=<target-server-node-name> export CLIENT_HOST_NAME=<target-client-node-name> If the target peer is not set, the last two of listed nodes will be selected as server and client, respectively. Run the test make sample-concheck Example output: # pod/multi-nic-iperf3-server created # pod/multi-nic-iperf3-server condition met # pod/multi-nic-iperf3-client created # pod/multi-nic-iperf3-client condition met # [ 5] local 192.168.0.66 port 46284 connected to 192.168.0.130 port 5201 # [ ID] Interval Transfer Bitrate Retr Cwnd # [ 5] 0.00-1.00 sec 121 MBytes 1.02 Gbits/sec 0 3.04 MBytes # [ 5] 1.00-2.00 sec 114 MBytes 954 Mbits/sec 0 3.04 MBytes # [ 5] 2.00-3.00 sec 115 MBytes 964 Mbits/sec 45 2.19 MBytes # [ 5] 3.00-4.00 sec 114 MBytes 954 Mbits/sec 45 1.67 MBytes # [ 5] 4.00-5.00 sec 114 MBytes 954 Mbits/sec 0 1.77 MBytes # - - - - - - - - - - - - - - - - - - - - - - - - - # [ ID] Interval Transfer Bitrate Retr # [ 5] 0.00-5.00 sec 577 MBytes 969 Mbits/sec 90 sender # [ 5] 0.00-5.04 sec 574 MBytes 956 Mbits/sec receiver # iperf Done. # pod \"multi-nic-iperf3-client\" deleted # pod \"multi-nic-iperf3-server\" deleted","title":"One-time peer-to-peer"},{"location":"user_guide/user/#one-time-all-to-all","text":"This is recommended for small cluster ( <10 HostInterfaces ) Run make concheck Example output: # serviceaccount/multi-nic-concheck-account created # clusterrole.rbac.authorization.k8s.io/multi-nic-concheck-cr created # clusterrolebinding.rbac.authorization.k8s.io/multi-nic-concheck-cr-binding created # job.batch/multi-nic-concheck created # Wait for job/multi-nic-concheck to complete # job.batch/multi-nic-concheck condition met # 2023/02/14 01:22:21 Config # W0214 01:22:21.976565 1 client_config.go:617] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. # 2023/02/14 01:22:23 2/2 servers successfully created # 2023/02/14 01:22:23 p-cni-operator-ipvlanl3-multi-nic-n7zf6-worker-2-zt5l5-serv: Pending ... # 2023/02/14 01:23:13 2/2 clients successfully finished # ########################################### # ## Connection Check: multi-nic-cni-operator-ipvlanl3 # ########################################### # FROM TO CONNECTED/TOTAL IPs BANDWIDTHs # multi-nic-n7zf6-worker-2-zt5l5 multi-nic-n7zf6-worker-2-zxw2n 2/2 [192.168.0.65 192.168.64.65] [ 1.05Gbits/sec 1.03Gbits/sec] # multi-nic-n7zf6-worker-2-zxw2n multi-nic-n7zf6-worker-2-zt5l5 2/2 [192.168.0.129 192.168.64.129] [ 934Mbits/sec 937Mbits/sec] # ########################################### # 2023/02/14 01:23:13 multi-nic-cni-operator-ipvlanl3 checked If the job takes longer than 50 minutes (mostly in large cluster), you will get error: timed out waiting for the condition on jobs/multi-nic-concheck Check the test progress directly: kubectl get po -w When the multi-nic-concheck job has completed, check the log: kubectl logs job/multi-nic-concheck If some connection failed, you may investigate the failure from log of the iperf3 client pod. Clean up the job make clean-concheck","title":"One-time all-to-all"}]}